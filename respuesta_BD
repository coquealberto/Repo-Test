Okay, vamos a abordar la integración de la conexión a la base de datos y la generación de los archivos de metadatos de K+ consultando el Data Warehouse.
Esto implica:
Añadir dependencias de base de datos: Incluir oracledb y pandas en requirements.txt.
Configurar credenciales y conexión a BD: Añadir usuario y contraseña de DB a .env y detalles de conexión (host, port, service_name) a config/settings.yaml.
Crear un nuevo paquete src/database/: Contendrá la lógica de conexión y consulta a la DB.
Implementar src/database/db_connection.py: Función para conectar a Oracle.
Implementar src/database/kplus_metadata_generator.py: Contendrá la lógica para consultar las tablas (GENESYS_CALL_DETAILS, GENESYS_CAMPAIGNS_FEEDBACK, GENESYS_INBOUND_CALLS, SUBSCRIBERS, SUBSCRIBER_ADDRESSES, etc.), extraer, estructurar los datos y guardarlos en archivos JSON ([conversationId].json) en data/k_plus_data/.
Actualizar main_workflow.py: Añadir un nuevo paso después del filtrado de transcripciones (Step 3) y antes de la carga de datos (Step 5, que se convertirá en Step 6) para llamar a la función de generación de metadatos.
Actualizar data_preparation/data_loader.py: Eliminar la lógica de generación de metadatos dummy y asegurarse de que cargue el archivo JSON generado por el nuevo paso.
Actualizar prompts en evaluation/item_rules.py: Asegurar que referencien correctamente la nueva estructura detallada de datos en k_plus_data_snapshot.
Importante: La implementación completa de todas las consultas a todas las tablas y el manejo detallado de todos los casos (CLOB, múltiples suscriptores, validaciones de fechas/códigos VALID en DB) es una tarea extensa. Proporcionaré la estructura y la lógica para los pasos principales, incluyendo la conexión, la búsqueda inicial del SUBSCRIBER_ID y la consulta a SUBSCRIBERS, SUBSCRIBER_ADDRESSES, SUBSCRIBER_PHONES, SUBSCRIBER_EMAILS con las validaciones especificadas. La parte de ACCOUNT/Producto/Cedente (ACC_SUBS_INT, DYNAMIC_FLAGS, FIN_ACCOUNT_BALANCES, etc.) requeriría aún más consultas encadenadas; la estructura estará ahí para añadirla.
1. Actualizar project_root/requirements.txt
Añade oracledb y pandas:
openai>=1.0.0
python-dotenv>=1.0.0
PyYAML>=6.0
requests>=2.0.0
pytz>=2023.0
tiktoken>=0.5.0
oracledb>=1.0.0 # Añadir
pandas>=2.0.0 # Añadir


Instala: pip install -r requirements.txt. (Puede requerir pasos adicionales para configurar el cliente de Oracle, consulta la documentación de oracledb).
2. Actualizar project_root/.env
Añade credenciales de base de datos:
# Variables de entorno para configuración sensible

AZURE_OPENAI_ENDPOINT="YOUR_AZURE_ENDPOINT"
AZURE_OPENAI_KEY="YOUR_AZURE_KEY"

# Credenciales de Genesys Cloud
GENESYS_CLOUD_CLIENT_ID="YOUR_GENESYS_CLIENT_ID"
GENESYS_CLOUD_CLIENT_SECRET="YOUR_GENESYS_CLOUD_CLIENT_SECRET"

# Credenciales de Base de Datos Oracle (Data Warehouse)
DB_USER="REPORTING"
DB_PASSWORD="YOUR_DB_PASSWORD"


3. Actualizar project_root/config/settings.yaml
Añade detalles de conexión a la BD y nombres de tablas.
# Configuración general del proyecto

# Configuración Azure OpenAI
openai:
  deployment_name: "TU_GPT_DEPLOYMENT_NAME"
  api_version: "2024-02-15-preview"

# Rutas de directorios (relativas a project_root o absolutas)
data_paths:
  base: "data/"
  raw_calls: "data/raw_calls/"
  raw_transcripts: "data/raw_transcripts/"
  whisper_transcripts: "data/whisper_transcripts/"
  k_plus_data: "data/k_plus_data/" # Directorio de salida para los JSON de metadatos generados
  results: "results/"

# Configuración del pipeline
pipeline:
  use_whisper: false
  items_to_evaluate:
    - "inicio_llamada_group"
    - "item_17_individual"
    - "item_20_individual"
    - "item_26_individual"
  download_date_range:
    start_datetime: "2024-04-25 00:00:00"
    end_datetime: "2024-04-25 23:59:59"
  download_datetime_format: "%Y-%m-%d %H:%M:%S"
  download_timezone: "Europe/Madrid"

# Configuración de la fuente de datos Genesys Cloud
genesys_cloud:
  environment: "mypurecloud.com"
  download:
    step_minutes: 60
    max_retries: 3
    retry_delay: 2
    search_query_params:
      types: ["transcripts"]
      returnFields: ["conversationId", "communicationId", "duration", "startTime"] # Añadir más si necesitas en K+ metadata
      query:
        - type: "EXACT"
          fields: ["language"]
          value: "es-es"
        - type: "EXACT"
          fields: ["mediaType"]
          value: "call"
        - type: "GREATER_THAN"
          fields: ["duration"]
          value: "15000"
      pageSize: 100

  filter:
    enabled: true
    accepted_wrap_up_codes: # COPIA AQUÍ LA LISTA DE CÓDIGOS WRAP-UP ACEPTADOS
      - "62c6d1ab-8b6e-448d-b71b-6febf9a76aea"
      - "63d7f77e-6d33-41e6-9b4c-9b5c786bb326"
      # ... resto de códigos

# Configuración de la Base de Datos Oracle (Data Warehouse)
database:
  oracle:
    host: "172.31.14.161"
    port: "1521"
    service_name: "DWH"
    # user y password en .env

# Nombres de tablas de Kollecto en el Data Warehouse Mirror (REPORTING schema)
kollecto_tables:
  call_details_outbound_primary: "REPORTING.GENESYS_CALL_DETAILS"
  call_details_outbound_fallback: "REPORTING.GENESYS_CAMPAIGNS_FEEDBACK" # SUBSCRIBER_ID singular aquí
  call_details_inbound: "REPORTING.GENESYS_INBOUND_CALLS"
  subscribers: "REPORTING.SUBSCRIBERS"
  subscriber_addresses: "REPORTING.SUBSCRIBER_ADDRESSES"
  subscriber_phones: "REPORTING.SUBSCRIBER_PHONES"
  subscriber_emails: "REPORTING.SUBSCRIBER_EMAILS"
  # Tablas adicionales para Item 5 (Producto, Cedente, Importe, Flags)
  acc_subs_int: "REPORTING.ACC_SUBS_INT"
  dynamic_flags: "REPORTING.DYNAMIC_FLAGS"
  dynamic_flags_hist: "REPORTING.DYNAMIC_FLAGS_HIST"
  fin_acc_balance_related_ids: "REPORTING.FIN_ACC_BALANCE_RELATED_IDS"
  fin_account_balances: "REPORTING.FIN_ACCOUNT_BALANCES"
  portfolios: "REPORTING.PORTFOLIOS"
  fin_client_products: "REPORTING.FIN_CLIENT_PRODUCTS" # Asumiendo que esta tabla existe
  fin_financial_products: "REPORTING.FIN_FINANCIAL_PRODUCTS"


# ... otras secciones de configuración


4. Crear el paquete src/database/
Crea la carpeta src/database/ y un archivo __init__.py dentro.
src/database/__init__.py
# Este archivo marca el directorio src/database como un paquete Python.
from .db_connection import conectar_a_oracle # Exponer la función de conexión
# Puedes exponer funciones clave de otros módulos de base de datos aquí
from .kplus_metadata_generator import KplusMetadataGenerator # Exponer la clase generadora



5. Implementar src/database/db_connection.py
Mueve la función de conexión a este archivo y haz que use config_manager.
src/database/db_connection.py
import oracledb
# Importar config_manager para obtener credenciales y detalles de conexión
from src.config import config_manager
import os # Para obtener credenciales sensibles de env

# Reemplazamos print con un logger básico para DB
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
db_logger = logging.getLogger(__name__)

def conectar_a_oracle():
    """
    Establece una conexión a la base de datos Oracle Data Warehouse.
    Obtiene los detalles de conexión de la configuración.
    """
    db_user = config_manager.get_env("DB_USER")
    db_password = config_manager.get_env("DB_PASSWORD")
    db_host = config_manager.get("database.oracle.host")
    db_port = config_manager.get("database.oracle.port")
    db_service_name = config_manager.get("database.oracle.service_name")

    if not all([db_user, db_password, db_host, db_port, db_service_name]):
        db_logger.error("Missing database connection details in config or environment variables.")
        print("Missing database connection details. Check .env (DB_USER, DB_PASSWORD) and config/settings.yaml.")
        return None

    try:
        # Construir la cadena de conexión (alternativa más segura que host/port/service_name separados en algunos casos)
        # dsn = f"{db_host}:{db_port}/{db_service_name}"
        # conn_oracle = oracledb.connect(user=db_user, password=db_password, dsn=dsn)

        # O usar ConnectParams como en tu código original
        params = oracledb.ConnectParams(host=db_host, port=db_port, service_name=db_service_name)
        conn_oracle = oracledb.connect(user=db_user, password=db_password, params=params)

        db_logger.info("Successfully connected to Oracle database.")
        print("Successfully connected to Oracle database.") # Print también para visibilidad en consola
        return conn_oracle
    except Exception as e:
        db_logger.error(f"Error connecting to Oracle database: {e}", exc_info=True)
        print(f"Error connecting to Oracle database: {str(e)}")
        return None

# La conexión se gestionará en el KplusMetadataGenerator o en main_workflow
# No es necesario mantener una conexión global aquí como en el código antiguo.
# La conexión debe abrirse, usarse y cerrarse adecuadamente.


6. Implementar src/database/kplus_metadata_generator.py

Este es el módulo principal para la generación de metadatos desde la BD.
src/database/kplus_metadata_generator.py


import os
import json
import oracledb
import pandas as pd # Usaremos pandas para ejecutar consultas y manejar resultados más fácilmente
from datetime import datetime # Para manejar fechas

from src.config import config_manager
from .db_connection import conectar_a_oracle # Importar la función de conexión

# Logger específico para este módulo
import logging
db_gen_logger = logging.getLogger(__name__)


class KplusMetadataGenerator:
    """
    Genera archivos de metadatos de K+ consultando el Data Warehouse.
    """
    def __init__(self):
        self.table_names = config_manager.get("kollecto_tables")
        self.kplus_data_output_dir = config_manager.get("data_paths.kplus_data")
        self.db_conn = None # Conexión a la BD, se abrirá/cerrará por batch o en el workflow

    def _get_connection(self):
        """Obtiene o establece la conexión a la BD."""
        if self.db_conn is None or not self.db_conn.ping():
            self.db_conn = conectar_a_oracle()
        return self.db_conn

    def _close_connection(self):
        """Cierra la conexión a la BD si está abierta."""
        if self.db_conn:
            try:
                self.db_conn.close()
                db_gen_logger.info("Database connection closed.")
            except Exception as e:
                db_gen_logger.error(f"Error closing database connection: {e}")
            self.db_conn = None


    def _query_table(self, query, params=None):
        """Ejecuta una consulta SQL y devuelve los resultados como DataFrame."""
        conn = self._get_connection()
        if conn is None:
            return pd.DataFrame() # Retornar DataFrame vacío si no hay conexión

        try:
            db_gen_logger.debug(f"Executing query: {query} with params: {params}")
            # Usamos pandas.read_sql para simplificar la obtención de resultados en DataFrame
            # Pasamos la conexión y los parámetros.
            # params={'id': some_value} si la query tiene WHERE CONVERSATION_ID = :id
            if params is None:
                 df = pd.read_sql(query, con=conn)
            else:
                 # pandas.read_sql maneja parámetros pasados como dict
                 df = pd.read_sql(query, con=conn, params=params)

            db_gen_logger.debug(f"Query returned {len(df)} rows.")
            return df
        except Exception as e:
            db_gen_logger.error(f"Error executing query: {query[:100]}...", {"params": params, "error": str(e)}, exc_info=True)
            return pd.DataFrame() # Retornar DataFrame vacío en caso de error

    def _find_subscriber_id(self, conversation_id, call_type):
        """
        Busca el/los SUBSCRIBER_ID(s) asociados a un ConversationId
        según el tipo de llamada y las tablas correspondientes.
        """
        subscriber_ids = []
        conn = self._get_connection()
        if conn is None:
            return None, "Database connection failed."

        db_gen_logger.info(f"Searching for SUBSCRIBER_ID for conversation {conversation_id} (Type: {call_type})")
        try:
            with conn.cursor() as cursor:
                if call_type == "saliente":
                    # 1. Buscar en GENESYS_CALL_DETAILS (saliente primaria)
                    query_primary = f"SELECT SUBSCRIBER_IDS FROM {self.table_names.get('call_details_outbound_primary')} WHERE CONVERSATION_ID = :conv_id"
                    cursor.execute(query_primary, [conversation_id])
                    result = cursor.fetchone()
                    if result and result[0]:
                        subscriber_ids_clob = result[0]
                        db_gen_logger.debug(f"Found SUBSCRIBER_IDS in primary outbound table: {subscriber_ids_clob}")
                        # Convertir CLOB a string y dividir por coma
                        if isinstance(subscriber_ids_clob, oracledb.LOB):
                            subscriber_ids_str = subscriber_ids_clob.read()
                        else: # Ya es string o None
                            subscriber_ids_str = str(subscriber_ids_clob) if subscriber_ids_clob is not None else ""

                        ids = [id.strip() for id in subscriber_ids_str.split(',') if id.strip()]
                        if len(ids) > 1:
                            db_gen_logger.warning(f"Multiple SUBSCRIBER_IDS found ({ids}) for conversation {conversation_id}. Skipping analysis.")
                            return None, "Multiple subscribers for conversation_id"
                        elif len(ids) == 1:
                            subscriber_ids = [ids[0]] # Encontramos un único ID en la tabla primaria
                        # Si len(ids) es 0 (CLOB vacío o solo comas), continuamos buscando en fallback

                    # 2. Si no se encontró en la primaria o era CLOB vacío, buscar en GENESYS_CAMPAIGNS_FEEDBACK (saliente fallback)
                    if not subscriber_ids:
                        db_gen_logger.info(f"SUBSCRIBER_IDS not found or empty in primary outbound table for {conversation_id}. Checking fallback table.")
                        query_fallback = f"SELECT SUBSCRIBER_ID FROM {self.table_names.get('call_details_outbound_fallback')} WHERE CONVERSATION_ID = :conv_id"
                        cursor.execute(query_fallback, [conversation_id])
                        result_fallback = cursor.fetchone()
                        if result_fallback and result_fallback[0]:
                            # En esta tabla, SUBSCRIBER_ID es singular y se espera que sea un número o string único
                            subscriber_id_fallback = str(result_fallback[0]).strip() # Convertir a string y limpiar
                            if subscriber_id_fallback:
                                subscriber_ids = [subscriber_id_fallback]
                                db_gen_logger.info(f"Found single SUBSCRIBER_ID '{subscriber_ids[0]}' in fallback outbound table for {conversation_id}.")


                elif call_type == "entrante":
                    # Buscar en GENESYS_INBOUND_CALLS (entrante)
                    query_inbound = f"SELECT SUBSCRIBER_IDS FROM {self.table_names.get('call_details_inbound')} WHERE CONVERSATION_ID = :conv_id"
                    cursor.execute(query_inbound, [conversation_id])
                    result = cursor.fetchone()
                    if result and result[0]:
                        subscriber_ids_clob = result[0]
                        db_gen_logger.debug(f"Found SUBSCRIBER_IDS in inbound table: {subscriber_ids_clob}")
                         # Convertir CLOB a string y dividir por coma
                        if isinstance(subscriber_ids_clob, oracledb.LOB):
                            subscriber_ids_str = subscriber_ids_clob.read()
                        else: # Ya es string o None
                            subscriber_ids_str = str(subscriber_ids_clob) if subscriber_ids_clob is not None else ""

                        ids = [id.strip() for id in subscriber_ids_str.split(',') if id.strip()]

                        if len(ids) > 1:
                            db_gen_logger.warning(f"Multiple SUBSCRIBER_IDS found ({ids}) for conversation {conversation_id}. Skipping analysis.")
                            return None, "Multiple subscribers for conversation_id"
                        elif len(ids) == 1:
                            subscriber_ids = [ids[0]] # Encontramos un único ID en la tabla inbound


                else:
                    db_gen_logger.warning(f"Unknown call type '{call_type}' for conversation {conversation_id}. Cannot determine table for subscriber ID lookup.")
                    return None, "Unknown call type."

        except Exception as e:
            db_gen_logger.error(f"Error finding SUBSCRIBER_ID for conversation {conversation_id}: {e}", exc_info=True)
            return None, f"Database query failed: {e}"

        if not subscriber_ids:
            db_gen_logger.warning(f"SUBSCRIBER_ID not found for conversation {conversation_id} in any relevant table.")
            return None, "Conversation ID not found in relevant DB tables or no subscriber ID associated."
        elif len(subscriber_ids) > 1:
             # Esto no debería pasar si la lógica anterior funciona, pero como chequeo final
             db_gen_logger.warning(f"Unexpectedly found more than one SUBSCRIBER_ID list ({subscriber_ids}) after processing CLOB for conversation {conversation_id}. Skipping analysis.")
             return None, "Multiple subscribers for conversation_id (after CLOB handling)."
        else:
            # Retornamos el único SUBSCRIBER_ID encontrado y un mensaje de éxito (None)
            db_gen_logger.info(f"Successfully found SUBSCRIBER_ID '{subscriber_ids[0]}' for conversation {conversation_id}.")
            return subscriber_ids[0], None # Retorna el subscriber_id único y no error


    def _get_subscriber_details(self, subscriber_id):
        """
        Obtiene detalles del suscriptor (nombre, DNI, fecha nacimiento)
        y maneja múltiples personas por subscriber_id.
        """
        details = []
        query = f"SELECT FIRST_NAME, LAST_NAME, SSN, BIRTH_DATE FROM {self.table_names.get('subscribers')} WHERE SUBSCRIBER_ID = :subscriber_id"
        df = self._query_table(query, params={'subscriber_id': subscriber_id})

        if df.empty:
            db_gen_logger.warning(f"No subscriber details found for SUBSCRIBER_ID: {subscriber_id}")
            return details # Retorna lista vacía si no se encuentra

        # Procesar cada fila (persona) asociada a este subscriber_id
        for index, row in df.iterrows():
            person_details = {}
            # Concatenar nombre y apellido si existen
            first_name = row.get('FIRST_NAME')
            last_name = row.get('LAST_NAME')
            full_name_parts = [name for name in [first_name, last_name] if pd.notna(name)]
            person_details['full_name'] = " ".join(full_name_parts) if full_name_parts else None

            # SSN
            ssn = row.get('SSN')
            person_details['ssn'] = str(ssn) if pd.notna(ssn) else None
            # Comprobar valor "000000X"
            if person_details['ssn'] and person_details['ssn'].startswith("000000"):
                db_gen_logger.debug(f"SSN '{person_details['ssn']}' for subscriber {subscriber_id} treated as undefined.")
                person_details['ssn'] = "Undefined" # Marcar como no definido si cumple el patrón

            # Birth Date (Date/Timestamp object)
            birth_date_dt = row.get('BIRTH_DATE')
            # Comprobar valor "1900-01-01" o null
            if pd.notna(birth_date_dt) and isinstance(birth_date_dt, datetime) and birth_date_dt.year != 1900:
                # Formatear a YYYY-MM-DD
                person_details['birth_date'] = birth_date_dt.strftime("%Y-%m-%d")
            else:
                db_gen_logger.debug(f"Birth date for subscriber {subscriber_id} is null, NaT, or 1900-01-01. Treated as undefined.")
                person_details['birth_date'] = None # Marcar como no definido

            details.append(person_details)

        db_gen_logger.info(f"Found {len(details)} person records for SUBSCRIBER_ID: {subscriber_id}")
        return details


    def _get_subscriber_contacts(self, subscriber_id):
        """
        Obtiene direcciones, teléfonos y emails válidos según criterios (VALID, LAST_UPDATE_DATE, SOURCE).
        """
        contacts = {
            'addresses': [],
            'phones': [],
            'emails': [],
            # Para Item 20 específico (VALID=6, SOURCE=82 phones)
            'phones_valid_6_source_82': []
        }
        conn = self._get_connection()
        if conn is None:
            return contacts # Retorna vacío si no hay conexión

        # Criterios comunes de filtro
        valid_codes_0_2 = (0, 2)
        valid_code_2 = (2,) # Solo para Ítem 20 base
        valid_code_6 = (6,) # Solo para Ítem 20 específico
        source_code_80 = 80
        source_code_82 = 82
        six_months_ago = datetime.now() - timedelta(days=180) # Aproximadamente 6 meses


        # --- Direcciones (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_addresses = f"""
        SELECT STREET, CITY, DISTRICT, POSTAL_CODE, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_addresses')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago
          AND SOURCE = :source_code
        """
        df_addresses = self._query_table(query_addresses, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_addresses.iterrows():
            address_parts = [row.get('STREET'), row.get('CITY'), row.get('DISTRICT'), row.get('POSTAL_CODE')]
            formatted_address = ", ".join([part for part in address_parts if pd.notna(part)])
            if formatted_address:
                 # Almacenar también el código VALID para referencia si es necesario, aunque el filtro ya lo aplicó
                contacts['addresses'].append({
                    'address': formatted_address,
                    'valid_code': row.get('VALID')
                })

        db_gen_logger.info(f"Found {len(contacts['addresses'])} valid addresses for SUBSCRIBER_ID: {subscriber_id}")


        # --- Teléfonos (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_phones_0_2 = f"""
        SELECT PHONE_NUMBER, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_phones')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago
          AND SOURCE = :source_code
        """
        df_phones_0_2 = self._query_table(query_phones_0_2, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_phones_0_2.iterrows():
             phone = row.get('PHONE_NUMBER')
             if pd.notna(phone):
                  contacts['phones'].append({
                      'phone_number': str(phone).strip(),
                      'valid_code': row.get('VALID')
                  })

        db_gen_logger.info(f"Found {len(contacts['phones'])} valid phones (0,2) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Emails (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_emails_0_2 = f"""
        SELECT EMAIL, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_emails')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago
          AND SOURCE = :source_code
        """
        df_emails_0_2 = self._query_table(query_emails_0_2, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_emails_0_2.iterrows():
             email = row.get('EMAIL')
             if pd.notna(email):
                  contacts['emails'].append({
                      'email': str(email).strip(),
                      'valid_code': row.get('VALID')
                  })
        db_gen_logger.info(f"Found {len(contacts['emails'])} valid emails (0,2) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Item 20 Specific Phones (VALID 6, Source 82) ---
        # La lógica de Item 20 también menciona phones con VALID=2 y SOURCE=80 para verificar altas correctas.
        # Esa info ya está en contacts['phones']. Aquí solo buscamos la condición específica VALID=6, SOURCE=82.
        query_phones_valid_6 = f"""
        SELECT PHONE_NUMBER, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_phones')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND SOURCE = :source_code
        """
        df_phones_valid_6 = self._query_table(query_phones_valid_6, params={'subscriber_id': subscriber_id, 'valid_codes': valid_code_6, 'source_code': source_code_82}) # Usar code 82 aquí

        for index, row in df_phones_valid_6.iterrows():
             phone = row.get('PHONE_NUMBER')
             if pd.notna(phone):
                  contacts['phones_valid_6_source_82'].append({
                      'phone_number': str(phone).strip(),
                      'valid_code': row.get('VALID'), # Será 6
                      'source_code': row.get('SOURCE') # Será 82
                  })
        db_gen_logger.info(f"Found {len(contacts['phones_valid_6_source_82'])} phones (VALID 6, SOURCE 82) for SUBSCRIBER_ID: {subscriber_id}")


        # Item 20 también necesita verificar si se dió de alta un dato nuevo con VALID=2
        # Esta verificación (si un dato *hablado* como nuevo aparece en la lista VALID=2)
        # se haría comparando la transcripción con contacts['phones'], ['emails'], ['addresses'] en el LLM/post-procesamiento,
        # no durante la consulta aquí. Solo necesitamos listar los contactos VALID=2.
        # Se podría filtrar contacts['phones'], ['emails'], ['addresses'] para solo incluir VALID=2
        # si la lógica de Item 20 solo mira los VALID=2, pero la regla dice "VALID = 2 o valor 0" para Item 6/7
        # y luego "unicamente aquellos que tengan VALID = 2" para Item 20.
        # La forma más limpia es retornar todos los VALID=0/2 aquí y dejar que el LLM/postprocessor aplique la lógica de VALID=2 vs VALID=0/2.
        # Sin embargo, el Item 20 específicamente "se hayan actualizado a raíz de esa llamada correctamente. Para ellos se cruzará con los datos de kollecto de números de teléfono con VALID = 6 y SOURCE = 82".
        # Esto implica que la verificación de ALTA CORRECTA (VALID=2) Y la verificación de NO PERMITIDO (VALID=6, SOURCE=82) se hacen con los datos de la BD.
        # La estructura del snapshot debe reflejar esto claramente.
        # Let's refine the contacts structure to separate VALID=2 for easier LLM use for Item 20 base.

        # Refined contact structure:
        refined_contacts = {
            'addresses_valid_0_2': contacts['addresses'],
            'phones_valid_0_2': contacts['phones'],
            'emails_valid_0_2': contacts['emails'],
            'phones_valid_6_source_82': contacts['phones_valid_6_source_82'],
             # Campos específicos para Item 20 base (VALID=2) - extraídos de los 0_2 listas
             'addresses_valid_2': [addr for addr in contacts['addresses'] if addr.get('valid_code') == 2],
             'phones_valid_2': [ph for ph in contacts['phones'] if ph.get('valid_code') == 2],
             'emails_valid_2': [em for em in contacts['emails'] if em.get('valid_code') == 2],
        }


        return refined_contacts


    def _get_account_and_financial_details(self, subscriber_id):
        """
        Obtiene detalles de cuenta, flags (Item 5), y balances financieros.
        (Implementación parcial - Lógica compleja a añadir)
        """
        financial_details = {
            'account_id': None,
            'flag_argumentario_prescripcion': {'flag_active': False, 'last_update_date': None}, # Item 5
            'importe_deuda': None, # Item 5
            'cedente': None, # Item 5
            'producto': None # Item 5
            # Puedes añadir listas si un subscriber tiene multiples cuentas/productos
        }
        conn = self._get_connection()
        if conn is None:
            return financial_details

        try:
            # 1. Get ACCOUNT_ID from ACC_SUBS_INT
            query_account_id = f"SELECT ACCOUNT_ID FROM {self.table_names.get('acc_subs_int')} WHERE SUBSCRIBER_ID = :subscriber_id"
            df_account = self._query_table(query_account_id, params={'subscriber_id': subscriber_id})
            if df_account.empty or len(df_account) > 1:
                db_gen_logger.warning(f"Account ID not found or multiple found for subscriber {subscriber_id}. Skipping financial details.")
                # Si hay múltiples cuentas, quizás necesites procesar todas. Por ahora, saltamos.
                return financial_details # Retorna detalles vacíos/nulos


            financial_details['account_id'] = df_account.iloc[0]['ACCOUNT_ID'] # Asumimos una única fila

            # 2. Check DYNAMIC_FLAGS for FLAG_ID 1501
            query_flags = f"""
            SELECT FLAG_ID, LAST_UPDATE_DATE
            FROM {self.table_names.get('dynamic_flags')}
            WHERE ACCOUNT_ID = :account_id AND FLAG_ID = 1501
            """
            df_flags = self._query_table(query_flags, params={'account_id': financial_details['account_id']})

            if not df_flags.empty:
                 # Encontrado en tablas activas
                 financial_details['flag_argumentario_prescripcion']['flag_active'] = True
                 # Coger la última fecha de actualización si hay varias filas (ej. si FLAG_ID no es PK única con ACCOUNT_ID)
                 # O asumir que solo hay una fila para FLAG_ID 1501 y account_id
                 if 'LAST_UPDATE_DATE' in df_flags.columns and not df_flags.empty:
                     # Encontrar la fecha más reciente si hay varias filas
                     latest_date = df_flags['LAST_UPDATE_DATE'].max()
                     if pd.notna(latest_date) and isinstance(latest_date, datetime):
                          financial_details['flag_argumentario_prescripcion']['last_update_date'] = latest_date.strftime("%Y-%m-%d") # Formato YYYY-MM-DD

            else:
                # Si no encontrado en flags activas, check DYNAMIC_FLAGS_HIST
                query_flags_hist = f"""
                SELECT FLAG_ID, LAST_UPDATE_DATE
                FROM {self.table_names.get('dynamic_flags_hist')}
                WHERE ACCOUNT_ID = :account_id AND FLAG_ID = 1501
                """
                df_flags_hist = self._query_table(query_flags_hist, params={'account_id': financial_details['account_id']})
                if not df_flags_hist.empty:
                     # Encontrado en historial, por lo tanto NO activa ahora
                     financial_details['flag_argumentario_prescripcion']['flag_active'] = False
                     # Podríamos coger la última fecha del historial si es relevante,
                     # pero la regla solo mira si "activa pero > 3 meses".
                     # Si está en historial, no está activa, la fecha del historial no importa para esa regla.
                     pass # No actualizar last_update_date si solo está en historial
                else:
                     # No encontrado ni en activa ni en historial
                     financial_details['flag_argumentario_prescripcion']['flag_active'] = False
                     # last_update_date ya es None

            db_gen_logger.info(f"Flag Argumentario Prescripcion (ID 1501) active status for account {financial_details['account_id']}: {financial_details['flag_argumentario_prescripcion']['flag_active']}")


            # --- Get Financial Balances and Product/Portfolio ---
            # This requires joining or chaining queries through related_ids table
            # This logic is more complex and depends heavily on how these tables are linked.
            # Placeholder for the chain of queries:
            # 3. Get FIN_ACCOUNT_BALANCE_ID from FIN_ACC_BALANCE_RELATED_IDS using ACCOUNT_ID
            # 4. Get AMOUNT_TO_PAY, PORTFOLIO_ID, FIN_CLIENT_PRODUCT_ID from FIN_ACCOUNT_BALANCES using FIN_ACCOUNT_BALANCE_ID
            # 5. Get Portfolio NAME from PORTFOLIOS using PORTFOLIO_ID
            # 6. Get FIN_FINANCIAL_PRODUCT_ID from FIN_CLIENT_PRODUCTS using FIN_CLIENT_PRODUCT_ID
            # 7. Get Product NAME from FIN_FINANCIAL_PRODUCTS using FIN_FINANCIAL_PRODUCT_ID

            # Example Placeholder:
            # Assuming we found one balance and can get its details
            # financial_details['importe_deuda'] = 1234.56 # Dummy
            # financial_details['cedente'] = "Dummy Cedente" # Dummy
            # financial_details['producto'] = "Dummy Producto" # Dummy
            db_gen_logger.warning("Financial balances, product, and portfolio lookup not fully implemented.")


        except Exception as e:
            db_gen_logger.error(f"Error getting financial details for subscriber {subscriber_id}: {e}", exc_info=True)
            # Retornar detalles como estaban antes del error
            return financial_details

        return financial_details


    def generate_metadata_for_call(self, call_info):
        """
        Genera el diccionario completo de metadatos de K+ para una sola llamada.

        Args:
            call_info (dict): Diccionario {'conversationId': str, 'communicationId': str, 'filepath': str}
                              para una llamada que pasó el filtro.

        Returns:
            dict or None: Diccionario con el snapshot de datos de K+ y otros metadatos,
                          o None si no se pudo generar (ej. no se encontró Subscriber ID).
        """
        conversation_id = call_info['conversationId']
        communication_id = call_info['communicationId']
        # Necesitamos el tipo de llamada ('saliente'/'entrante') para buscar el subscriber_id
        # Esto debe venir del JSON de transcripción raw, que ya se cargó en el filtro,
        # pero no lo tenemos aquí directamente.
        # Opción A: Modificar filter_transcriptions para que también devuelva call_type.
        # Opción B: Volver a abrir el archivo raw_transcript_filepath aquí para leer el call_type.
        # Opción A es más limpia. Vamos a modificar filter_transcriptions.
        # Por ahora, asumimos que call_info ahora incluye 'call_type'.

        call_type = call_info.get('call_type') # Asumimos que el filtro ahora añade esto
        if not call_type:
             db_gen_logger.error(f"Call type not found in call_info for conversation {conversation_id}. Cannot proceed.")
             return None, "Call type missing."

        db_gen_logger.info(f"Generating K+ metadata for conversation {conversation_id} (Type: {call_type})")

        # 1. Encontrar SUBSCRIBER_ID
        subscriber_id, find_error = self._find_subscriber_id(conversation_id, call_type)

        if find_error:
            db_gen_logger.warning(f"Skipping K+ metadata generation for {conversation_id} due to: {find_error}")
            return None, find_error # No se puede generar metadatos sin un único Subscriber ID

        db_gen_logger.info(f"Found main SUBSCRIBER_ID: {subscriber_id}")

        # Inicializar el snapshot de datos de K+
        k_plus_data_snapshot = {}
        k_plus_data_snapshot['main_subscriber_id'] = subscriber_id
        k_plus_data_snapshot['call_type'] = call_type # Añadir tipo de llamada al snapshot también

        # 2. Obtener detalles del suscriptor (nombre, DNI, fecha nacimiento, etc.)
        k_plus_data_snapshot['subscribers'] = self._get_subscriber_details(subscriber_id)
        if not k_plus_data_snapshot['subscribers']:
             db_gen_logger.warning(f"No subscriber details found for SUBSCRIBER_ID {subscriber_id}. Metadatos incompletos.")
             # Decidir si esto es un fallo crítico. La regla dice "si no se consigue extraer ningún resultado... no se realizará el análisis".
             # Esto podría significar que si no hay *ningún* resultado en la búsqueda inicial O si no hay *detalles* de ese subscriber ID, se salta.
             # Si _get_subscriber_details retorna [], significa que el subscriber ID existe pero no tiene fila en SUBSCRIBERS.
             # Consideremos esto un fallo si no hay al menos 1 registro de persona para el subscriber ID.
             # Si no hay personas, no hay nombre, DNI, fecha nacimiento, etc.
             if not k_plus_data_snapshot['subscribers']:
                  db_gen_logger.warning(f"No person details found for SUBSCRIBER_ID {subscriber_id}. Skipping analysis.")
                  return None, f"No person details found for subscriber ID {subscriber_id}."


        # 3. Obtener información de contacto (direcciones, teléfonos, emails válidos)
        contacts = self._get_subscriber_contacts(subscriber_id)
        k_plus_data_snapshot.update(contacts) # Añadir las claves de contactos al snapshot

        # 4. Obtener detalles de cuenta, flags, financieros (Implementación parcial)
        # Esto requiere la cadena de consultas _get_account_and_financial_details
        # k_plus_data_snapshot.update(self._get_account_and_financial_details(subscriber_id))
        db_gen_logger.warning("Skipping full financial details lookup (_get_account_and_financial_details) - not fully implemented.")
        # Añadir placeholders si estos campos se usan en prompts y no se obtienen
        k_plus_data_snapshot['account_id'] = None
        k_plus_data_snapshot['flag_argumentario_prescripcion'] = {'flag_active': False, 'last_update_date': None}
        k_plus_data_snapshot['importe_deuda'] = None
        k_plus_data_snapshot['cedente'] = None
        k_plus_data_snapshot['producto'] = None
        k_plus_data_snapshot['is_prescription_flag_recent'] = False # Calcular esto aquí si se obtiene last_update_date real


        # --- Calcular flags/datos derivados que el LLM necesita si no se hacen en data_loader ---
        # La lógica de Item 5 (flag activa y fecha < 3 meses) se puede calcular aquí
        # si se obtienen los datos de flags reales. Si no, data_loader la calculará
        # a partir de los datos dummy/placeholder en el snapshot.
        # Si _get_account_and_financial_details estuviera completa:
        # flag_info = k_plus_data_snapshot.get('flag_argumentario_prescripcion', {})
        # if flag_info.get('flag_active') and flag_info.get('last_update_date'):
        #      try:
        #          last_update_dt = datetime.strptime(flag_info['last_update_date'], "%Y-%m-%d")
        #          if datetime.now() - last_update_dt < timedelta(days=90):
        #              k_plus_data_snapshot['is_prescription_flag_recent'] = True
        #          else:
        #              k_plus_data_snapshot['is_prescription_flag_recent'] = False
        #      except ValueError:
        #           db_gen_logger.warning(f"Could not parse flag date {flag_info['last_update_date']} for {conversation_id}. Cannot calculate recency.")
        #           k_plus_data_snapshot['is_prescription_flag_recent'] = False
        # else:
        #      k_plus_data_snapshot['is_prescription_flag_recent'] = False


        # Retornar el snapshot y None para el error si fue exitoso
        db_gen_logger.info(f"Successfully generated K+ metadata snapshot for conversation {conversation_id}.")
        return k_plus_data_snapshot, None


    def generate_kplus_metadata_files(self, filtered_calls_info_list: list):
        """
        Procesa una lista de llamadas filtradas, genera metadatos de K+ para cada una,
        y los guarda en archivos JSON.

        Args:
            filtered_calls_info_list (list): Lista de diccionarios
                                           {'conversationId': str, 'communicationId': str, 'filepath': str, 'call_type': str}
                                           (Nota: 'call_type' debe ser añadido por el filtro).

        Returns:
            list: Una lista de diccionarios {'conversationId': str, 'communicationId': str, 'filepath': str}
                  para las llamadas para las que se pudo generar el archivo de metadatos.
        """
        print("\n--- Step (New): K+ Metadata Generation (Database Lookup) ---")
        generated_metadata_calls_info = []
        skipped_calls_count = 0

        # Asegurar directorio de salida para los archivos de metadatos
        os.makedirs(self.kplus_data_output_dir, exist_ok=True)
        db_gen_logger.info(f"Saving K+ metadata JSON files to: {self.kplus_data_output_dir}")


        # Conectar a la BD una vez para el lote (gestión de conexión en _get_connection)
        initial_conn = self._get_connection()
        if initial_conn is None:
             print("Database connection failed. Cannot generate K+ metadata files. Exiting step.")
             # Devolvemos la lista vacía
             return generated_metadata_calls_info

        total_calls = len(filtered_calls_info_list)
        print(f"Attempting to generate K+ metadata for {total_calls} calls.")

        for index, call_info in enumerate(filtered_calls_info_list):
            conversation_id = call_info.get('conversationId')
            # communication_id = call_info.get('communicationId') # No necesario directamente aquí, pero útil en logs
            # call_type = call_info.get('call_type') # Necesario! Asumimos que el filtro lo añade

            if not conversation_id:
                 db_gen_logger.warning(f"Skipping item {index+1}/{total_calls}: Missing conversationId in call info. Info: {call_info}")
                 skipped_calls_count += 1
                 continue

            # --- Generar el snapshot de metadatos para la llamada ---
            # generate_metadata_for_call retorna el snapshot o None, y un mensaje de error
            k_plus_data_snapshot, error_message = self.generate_metadata_for_call(call_info)


            if k_plus_data_snapshot is not None:
                # --- Guardar el snapshot en un archivo JSON ---
                # La estructura del archivo JSON será simplemente el snapshot
                output_filepath = os.path.join(self.kplus_data_output_dir, f"{conversation_id}.json")

                try:
                     # Guardar el diccionario del snapshot directamente en el archivo
                     with open(output_filepath, 'w', encoding='utf-8') as f:
                          json.dump(k_plus_data_snapshot, f, indent=4, ensure_ascii=False)
                     db_gen_logger.info(f"Successfully saved K+ metadata JSON for {conversation_id} to {output_filepath}")

                     # Si se guardó exitosamente, añadir esta llamada a la lista de procesados
                     generated_metadata_calls_info.append(call_info) # Añadimos el dict original de call_info

                except Exception as e:
                     db_gen_logger.error(f"Error saving K+ metadata JSON for {conversation_id} to {output_filepath}: {e}", exc_info=True)
                     skipped_calls_count += 1

            else:
                 # La generación de metadatos falló para esta llamada
                 db_gen_logger.warning(f"Skipping call {conversation_id} metadata generation due to: {error_message}")
                 skipped_calls_count += 1

        # Cerrar la conexión al finalizar el lote
        self._close_connection()

        print(f"\n--- K+ Metadata Generation Summary ---")
        print(f"Total calls attempted: {total_calls}")
        print(f"Metadata files successfully generated: {len(generated_metadata_calls_info)}")
        print(f"Calls skipped: {skipped_calls_count}")
        print("------------------------------------")

        return generated_metadata_calls_info # Retorna la lista de llamadas (con info) que tienen archivo de metadatos



7. Actualizar src/database/db_connection.py
Este archivo ya se implementó en el paso 5 anterior.
8. Actualizar src/data_preparation/transcript_filter.py
Modificamos la función de filtro para que también extraiga y devuelva el call_type (saliente/entrante) de la transcripción raw, ya que la lógica de la BD lo necesita. Asumimos que está en el initialDirection del primer participante.

import os
import json
from datetime import datetime
# Importar config_manager para obtener configuración y rutas
from src.config import config_manager

# Reemplazamos logging con un logger específico para este módulo
import logging
filter_logger = logging.getLogger(__name__)
# Puedes configurar el nivel en el main_workflow o en logging_config si lo implementas
if not filter_logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') # Configuración básica si no existe


def filter_transcriptions(transcript_filepaths: list):
    """
    Filtra archivos de transcripción JSON basados en su wrapUpCode (si filtro habilitado).
    Elimina los archivos que no cumplen el criterio si el filtro está habilitado.
    Extrae IDs y call_type para los archivos que pasan o si el filtro está deshabilitado.

    Args:
        transcript_filepaths (list): Lista de rutas de archivos de transcripción JSON raw descargados.

    Returns:
        list: Una lista de diccionarios, donde cada diccionario contiene
              {'conversationId': str, 'communicationId': str, 'filepath': str, 'call_type': str}
              para los archivos que pasaron el filtro o fueron incluidos si el filtro está deshabilitado.
    """
    print("\n--- Data Preparation (Transcript Filtering) ---")
    filter_config = config_manager.get("genesys_cloud.filter", {})
    filter_enabled = filter_config.get("enabled", False)

    accepted_codes = set(filter_config.get("accepted_wrap_up_codes", []))

    processed_files_info = [] # Lista para almacenar los dicts {IDs, filepath, call_type}


    if not filter_enabled:
        filter_logger.info("Transcript filtering is disabled in settings.yaml. Processing all found files.")
        # Si el filtro está deshabilitado, simplemente cargamos cada archivo para obtener los IDs y call_type
        for filepath in transcript_filepaths:
             filename = os.path.basename(filepath)
             conversation_id = os.path.splitext(filename)[0]

             try:
                  with open(filepath, 'r', encoding='utf-8') as f:
                       transcript_data_partial = json.load(f)
                       comm_id = transcript_data_partial.get("communicationId", conversation_id)
                       # Extraer call_type (saliente/entrante) del primer participante si está disponible
                       call_type = "Desconocido"
                       participants = transcript_data_partial.get("participants", [])
                       if participants:
                            initial_direction = participants[0].get("initialDirection")
                            if initial_direction:
                                 call_type = "saliente" if initial_direction == "outbound" else "entrante"

                  processed_files_info.append({
                       'conversationId': conversation_id,
                       'communicationId': comm_id,
                       'filepath': filepath, # Ruta al archivo JSON raw
                       'call_type': call_type
                  })
                  filter_logger.info(f"Filter disabled. Including file {filename}", {"conversationId": conversation_id, "communicationId": comm_id, "call_type": call_type})

             except (FileNotFoundError, json.JSONDecodeError, Exception) as e:
                  filter_logger.error(f"Error reading file {filepath} while filter is disabled: {e}. Skipping.", {"conversationId": conversation_id})

        print(f"Filter disabled. {len(processed_files_info)} files included.")
        return processed_files_info


    # --- Lógica de Filtrado (si está habilitado) ---
    if not accepted_codes:
        filter_logger.warning('Filter enabled but no accepted wrap_up_codes defined in settings.yaml.')
        print("No accepted wrap_up_codes defined in config. Filtering enabled but no codes to check against.")
        return [] # Si el filtro está habilitado pero sin códigos, no pasa nada.

    print(f"Filtering {len(transcript_filepaths)} transcripts based on wrap-up codes.")
    filter_logger.info(f"Accepted wrap-up codes: {list(accepted_codes)}")


    conteo_codigos_aceptados = {codigo: 0 for codigo in accepted_codes}
    conteo_filtrados = 0
    conteo_error = 0

    for filepath in transcript_filepaths:
        filename = os.path.basename(filepath)
        conversation_id = os.path.splitext(filename)[0]
        communication_id = conversation_id # Valor por defecto si communicationId no se encuentra
        call_type = "Desconocido"
        wrap_up_code = None

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                transcript_data = json.load(f)

            # Extraer IDs y call_type del JSON
            communication_id = transcript_data.get("communicationId", conversation_id)
            participants = transcript_data.get("participants", [])
            if participants:
                 initial_direction = participants[0].get("initialDirection")
                 if initial_direction:
                      call_type = "saliente" if initial_direction == "outbound" else "entrante"

            # Buscar wrapUpCode en el participante con participantPurpose "agent"
            for participant in participants:
                if participant.get("participantPurpose") == "agent":
                    wrap_up_code = participant.get("wrapUpCode")
                    # El campo wrapUpCode puede ser null si no se asignó
                    break

            if wrap_up_code is None:
                filter_logger.info(f"Filtering file {filename}: No 'wrapUpCode' found for an 'agent' participant.", {"conversation_id": conversation_id})
                os.remove(filepath)
                conteo_filtrados += 1
            elif wrap_up_code in accepted_codes:
                filter_logger.info(f"File {filename} PASSED filter. Wrap-up code: {wrap_up_code}", {"conversation_id": conversation_id, "communicationId": communication_id})
                passed_filter_info.append({
                     'conversationId': conversation_id,
                     'communicationId': communication_id,
                     'filepath': filepath,
                     'call_type': call_type # Añadir call_type aquí
                })
                conteo_codigos_aceptados[wrap_up_code] += 1
            else:
                filter_logger.info(f"Filtering file {filename}: Wrap-up code '{wrap_up_code}' NOT in accepted list.", {"conversation_id": conversation_id, "communicationId": communication_id})
                os.remove(filepath)
                conteo_filtrados += 1

        except (FileNotFoundError, json.JSONDecodeError, Exception) as e:
            filter_logger.error(f"Error processing file {filepath} during filtering: {e}. Deleting.", {"conversation_id": conversation_id, "communicationId": communication_id, "error": str(e)})
            try:
                 if os.path.exists(filepath):
                    os.remove(filepath)
            except OSError:
                 pass
            conteo_error += 1
            conteo_filtrados += 1

    print("\n--- Transcript Filtering Summary ---")
    print(f"Total files processed (initial): {len(transcript_filepaths)}")
    print(f"Files passed filter: {len(passed_filter_info)}")
    print(f"Files filtered (wrap-up not accepted/missing or error): {conteo_filtrados}")
    print(f"Files skipped due to read/parse error during filter: {conteo_error}")
    print("Counts for accepted wrap-up codes (among those that passed):")
    for codigo, conteo in conteo_codigos_aceptados.items():
        print(f"  - '{codigo}': {conteo}")
    print("------------------------------------")


    return passed_filter_info

9. Actualizar src/data_preparation/data_loader.py
Eliminamos la generación de metadatos dummy (la lógica ahora está en KplusMetadataGenerator) y nos aseguramos de que cargue el archivo JSON de metadatos generado.

import os
import json
from datetime import datetime, timedelta
from pytz import timezone

from src.config import config_manager

# Logger específico para este módulo
import logging
data_loader_logger = logging.getLogger(__name__)
# Configuración básica si no existe
if not data_loader_logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


def extract_formatted_transcript_text(transcript_json_data):
    """
    Extrae y formatea el texto de la transcripción de un JSON raw de Genesys,
    intercalando los hablantes.
    """
    frases_formateadas = []
    hablante_anterior = None

    transcripts_array = transcript_json_data.get("transcripts", [])
    for transcript_entry in transcripts_array:
        if 'VOICE_TRANSCRIPTION' in transcript_entry.get('features', []):
            phrases = transcript_entry.get('phrases', [])
            for phrase in phrases:
                texto = phrase.get("text", "").strip()
                purpose = phrase.get("participantPurpose", "")

                if purpose == "internal":
                    hablante = "Agente"
                elif purpose == "external":
                    hablante = "Interlocutor"
                else:
                    hablante = "Desconocido"

                if not texto:
                    continue

                if frases_formateadas and hablante == hablante_anterior:
                    frases_formateadas[-1] += f" {texto}"
                else:
                    frases_formateadas.append(f"{hablante}: {texto}")
                    hablante_anterior = hablante

    return "\n".join(frases_formateadas)


def load_call_data_for_evaluation(call_id: str, raw_transcript_filepath: str, use_whisper: bool = True):
    """
    Carga y prepara todos los datos necesarios para la evaluación de una sola llamada.

    Args:
        call_id (str): El ID único de la llamada (conversationId).
        raw_transcript_filepath (str): Ruta al archivo JSON de la transcripción raw descargada.
        use_whisper (bool): Si intentar cargar la transcripción de Whisper (.txt) en lugar de usar el texto del JSON raw.

    Returns:
        dict: Diccionario `call_data` con 'transcript' (texto plano) y 'call_metadata',
              o None si los archivos necesarios no se encuentran o fallan al cargar.
    """
    data_loader_logger.info(f'Loading data for call ID: {call_id}')

    data_paths = config_manager.get("data_paths")
    whisper_transcript_dir = data_paths["whisper_transcripts"]
    k_plus_data_dir = data_paths["k_plus_data"] # Directorio donde están los JSON de metadatos generados

    raw_transcript_json = None
    transcript_content = None # El texto plano final a usar para evaluación

    # --- 1. Cargar y Parsear el JSON de Transcripción Raw ---
    # Esto sigue siendo necesario para obtener metadata incluso si se usa Whisper para el texto
    try:
        with open(raw_transcript_filepath, 'r', encoding='utf-8') as f:
            raw_transcript_json = json.load(f)
        data_loader_logger.info(f'Loaded raw transcript JSON from: {raw_transcript_filepath}')

        # Extraer el texto plano formateado del JSON raw
        transcript_content_raw_formatted = extract_formatted_transcript_text(raw_transcript_json)
        if not transcript_content_raw_formatted:
             data_loader_logger.warning(f'Extracted empty text content from raw transcript JSON for call {call_id}. File: {raw_transcript_filepath}')


    except FileNotFoundError:
        data_loader_logger.error(f'Raw transcript file not found for call {call_id} at {raw_transcript_filepath}. This should not happen if filter passed it.', {"call_id": call_id})
        return None
    except json.JSONDecodeError:
        data_loader_logger.error(f'Error decoding JSON for raw transcript: {raw_transcript_filepath}.', {"call_id": call_id})
        return None
    except Exception as e:
        data_loader_logger.error(f'An unexpected error occurred loading raw transcript {raw_transcript_filepath}: {e}', {"call_id": call_id})
        return None

    # --- 2. Intentar cargar Transcripción Whisper si aplica ---
    if use_whisper:
        whisper_filepath = os.path.join(whisper_transcript_dir, f"{call_id}.txt")
        try:
            with open(whisper_filepath, 'r', encoding='utf-8') as f:
                transcript_content_whisper = f.read()
            if transcript_content_whisper.strip(): # Usar la de Whisper si no está vacía
                transcript_content = transcript_content_whisper.strip()
                data_loader_logger.info(f'Loaded Whisper transcript from: {whisper_filepath}. Using for evaluation.')
            else:
                 # Si el archivo Whisper está vacío, usar la transcripción formateada del JSON raw
                 transcript_content = transcript_content_raw_formatted
                 data_loader_logger.warning(f'Whisper transcript file {whisper_filepath} is empty. Falling back to raw transcript text.')

        except FileNotFoundError:
            # Si no se encuentra el archivo Whisper, usar la transcripción del JSON raw
            transcript_content = transcript_content_raw_formatted
            data_loader_logger.warning(f'Whisper transcript file not found for call {call_id} at {whisper_filepath}. Falling back to raw transcript text.')
        except Exception as e:
             # Otros errores al cargar Whisper, usar la del JSON raw
             transcript_content = transcript_content_raw_formatted
             data_loader_logger.error(f'Error loading Whisper transcript {whisper_filepath}: {e}. Falling back to raw transcript text.', {"call_id": call_id})
    else:
        # Si use_whisper es False, usamos directamente la transcripción formateada del JSON raw
        transcript_content = transcript_content_raw_formatted
        data_loader_logger.info('use_whisper is False. Using raw transcript text for evaluation.')

    # Si a pesar de todo la transcripción está vacía, no podemos evaluar
    if not transcript_content:
         data_loader_logger.error(f'Final transcript content is empty for call {call_id}. Cannot evaluate.')
         return None


    # --- 3. Cargar Metadatos Generados de K+ ---
    # Este archivo DEBE existir si el paso de generación de metadatos fue exitoso.
    # La lógica de generación dummy (si no se encuentra en DB) está ahora en el KplusMetadataGenerator.
    metadata_filepath = os.path.join(kplus_data_dir, f"{call_id}.json")
    k_plus_data_snapshot = None # Inicializar como None
    call_metadata = {} # Inicializar el diccionario de metadatos que se pasará al LLM

    try:
        with open(metadata_filepath, 'r', encoding='utf-8') as f:
            # El archivo JSON generado contiene directamente el snapshot de K+
            k_plus_data_snapshot = json.load(f)
            data_loader_logger.info(f'Loaded K+ metadata snapshot from: {metadata_filepath}')

    except FileNotFoundError:
         # Esto no debería pasar si el pipeline está correcto y el generador tuvo éxito.
         # Si pasa, significa que el generador falló para esta llamada o hubo un error de ruta.
         data_loader_logger.error(f"K+ metadata snapshot file NOT found for call {call_id} at {metadata_filepath}. This call was likely skipped in K+ metadata generation.", {"call_id": call_id})
         # No podemos continuar con la evaluación si los metadatos de K+ críticos faltan
         return None
    except json.JSONDecodeError:
         data_loader_logger.error(f"Error decoding JSON from K+ metadata snapshot file {metadata_filepath} for call {call_id}. Cannot proceed.", {"call_id": call_id})
         return None
    except Exception as e:
         data_loader_logger.error(f'An unexpected error occurred loading K+ metadata snapshot for call {call_id}: {e}. Cannot proceed.', {"call_id": call_id})
         return None

    # --- 4. Ensamblar call_metadata final fusionando info del JSON Raw y K+ Snapshot ---
    # Empezamos con los datos extraídos del JSON raw (IDs, tipo llamada, wrap-up)
    call_metadata['call_id'] = raw_transcript_json.get('conversationId', call_id) # ConversationId
    call_metadata['conversationId'] = call_metadata['call_id']
    call_metadata['communicationId'] = raw_transcript_json.get('communicationId', call_id)
    call_metadata['call_type'] = raw_transcript_json.get('participants', [{}])[0].get('initialDirection', 'unknown') # Extraer del primer participante
    if isinstance(call_metadata['call_type'], str):
         call_metadata['call_type'] = 'saliente' if call_metadata['call_type'].lower() == 'outbound' else 'entrante' if call_metadata['call_type'].lower() == 'inbound' else 'Desconocido'
    else:
         call_metadata['call_type'] = 'Desconocido' # Asegurar string

    # Extraer wrap-up info del JSON raw y usar el mapeo (el filtro ya verificó la existencia del código)
    wrap_up_code_raw = None
    participants = raw_transcript_json.get("participants", [])
    for participant in participants:
         if participant.get("participantPurpose") == "agent":
              wrap_up_code_raw = participant.get("wrapUpCode")
              break
    call_metadata['wrap_up_code'] = wrap_up_code_raw
    wrapup_codes_mapping = config_manager.get_ref_data("wrapup_codes")
    call_metadata['wrap_up_info'] = wrapup_codes_mapping.get(wrap_up_code_raw, {}) if wrapup_codes_mapping and wrap_up_code_raw else {}


    # Añadir otros metadatos útiles del JSON raw si están disponibles
    call_metadata['raw_duration_ms'] = raw_transcript_json.get('duration', {}).get('milliseconds') # Duración del array transcripts[].duration
    call_metadata['raw_start_time_ms'] = raw_transcript_json.get('startTime') # StartTime del array transcripts[]
    call_metadata['conversationStartTime'] = raw_transcript_json.get('conversationStartTime') # StartTime global de la conversación
    call_metadata['conversationDuration'] = raw_transcript_json.get('conversationDuration') # Duración total de la conversación en ms
    # Convertir duración a minutos si es útil
    call_metadata['duration_minutes'] = call_metadata['conversationDuration'] / 60000.0 if isinstance(call_metadata['conversationDuration'], (int, float)) else 0


    # Fusionar los metadatos cargados del archivo K+ (si existían y no se generaron dummy)
    # Estos podrían incluir flags a nivel de llamada (incidencia K+, autorizado expreso, etc.)
    # que no vienen directamente del JSON de transcripción.
    # Asumimos que el archivo K+ generado por el generador contiene solo el snapshot,
    # pero si tu diseño incluye más metadatos en ese archivo, fusiónalos aquí.
    # Por ahora, añadimos campos que asumimos vienen de metadata externa,
    # o que el generador de metadatos podría añadir a nivel principal si no vienen del snapshot.
    # Estos campos deben estar en el diccionario cargado del archivo K+ si existía.
    # Si el generador de metadatos siempre guarda un dict como {'k_plus_data_snapshot': {...}, 'k_plus_incident': ..., 'is_authorised_express': ...}
    # entonces simplemente fusionamos esas claves aquí.
    # Para simplicidad, asumimos que los metadatos no relacionados con el snapshot (como incidence_k+, etc.)
    # SIEMPRE vendrán en el diccionario cargado del archivo K+ si existía,
    # o serán valores por defecto si el archivo K+ no existió y se generó dummy snapshot.
    # Si el generador de metadatos solo guarda el snapshot, necesitarías cargar estos
    # metadatos adicionales de otra fuente o tenerlos en un archivo K+ separado
    # que no sea solo el snapshot.
    # Para la POC, asumimos que KplusMetadataGenerator pone estos campos a nivel principal
    # del dict JSON que guarda, si los puede obtener de otras fuentes o si son flags fijos.
    # O que vienen en el mismo JSON que k_plus_data_snapshot si el archivo existía.
    # Si KplusMetadataGenerator solo guarda el snapshot, necesitarías añadir esos campos
    # aquí manualmente con valores por defecto si no los extraes de otra parte.
    # Por simplicidad, asumiremos que el generador pone flags como is_authorised_express, etc.
    # a nivel principal del JSON que guarda.
    # Si el generador *falló* para una llamada, k_plus_data_snapshot es None aquí (manejado arriba).
    # Si el generador *tuvo éxito* pero no encontró algunos datos (ej. no incidencias), esos campos
    # simplemente no estarán en el snapshot o serán None. El prompt_data_builder manejará defaults.

    # Añadir el snapshot de datos de K+ (cargado)
    call_metadata['k_plus_data_snapshot'] = k_plus_data_snapshot if k_plus_data_snapshot is not None else {}

    # Añadir la ruta de la transcripción cargada y si se usó Whisper para referencia
    call_metadata['raw_transcript_filepath'] = raw_transcript_filepath
    call_metadata['used_whisper_transcript'] = use_whisper
    if use_whisper:
        call_metadata['whisper_transcript_filepath'] = os.path.join(whisper_transcript_dir, f"{call_id}.txt")


    # --- 5. Ensamblar el diccionario final para evaluación ---
    call_data = {
        "transcript": transcript_content, # El texto (Whisper si aplica, raw formateado si no)
        "call_metadata": call_metadata # Todos los metadatos y K+
    }

    data_loader_logger.info(f'Data prepared for call {call_id}.')
    return call_data


10. Actualizar src/main_workflow.py (Insertar paso de Generación de Metadatos)
Insertamos la llamada a KplusMetadataGenerator y ajustamos el flujo.

import os
import glob
from datetime import datetime, timedelta
from pytz import timezone
import pytz

from src.config import config_manager
from src.data_acquisition import downloader
from src.audio_processing import whisper_processor # Usado solo si use_whisper es True
from src.data_preparation import data_loader, transcript_filter
from src.evaluation import evaluate_with_llm, apply_post_processing, EVALUATION_STRUCTURE
from src.results import results_handler
# Importar el nuevo generador de metadatos
from src.database import KplusMetadataGenerator, conectar_a_oracle # Importar la clase y la función de conexión (aunque el generador la usa internamente)


def main():
    """
    Función principal que ejecuta el pipeline de evaluación.
    """
    print("--- Starting Automated Call Evaluation Pipeline ---")

    # 1. Cargar Configuración
    config = config_manager

    data_paths = config.get("data_paths")
    pipeline_config = config.get("pipeline")
    genesys_config = config.get("genesys_cloud")
    db_config = config.get("database") # Obtener config BD

    if not all([data_paths, pipeline_config, genesys_config, db_config]):
        print("Error: Failed to load essential configuration (data_paths, pipeline, genesys_cloud, or database). Exiting.")
        return

    # Asegurar directorios existen
    for path_key in data_paths.values():
        if isinstance(path_key, str):
             os.makedirs(path_key, exist_ok=True)

    # --- Inicializar contadores globales de tokens ---
    total_input_tokens = 0
    total_output_tokens = 0
    token_counts_by_eval_key = {key: {"input": 0, "output": 0} for key in EVALUATION_STRUCTURE.keys()}


    # 2. Adquisición de Datos (Genesys Transcriptions Download)
    print("\n--- Step 1: Data Acquisition (Genesys Transcriptions Download) ---") # Renumeramos los pasos
    download_date_range_config = pipeline_config.get("download_date_range")

    raw_transcript_filepaths = [] # Inicializar siempre la lista

    if download_date_range_config and download_date_range_config.get("start_datetime") and download_date_range_config.get("end_datetime"):
        start_datetime_str = download_date_range_config["start_datetime"]
        end_datetime_str = download_date_range_config["end_datetime"]

        try:
             raw_transcript_filepaths = downloader.download_transcriptions_batch(start_datetime_str, end_datetime_str)

        except Exception as e:
             print(f"An error occurred during the download step: {e}")
             # raw_transcript_filepaths seguirá siendo []


        if raw_transcript_filepaths:
             print(f"Successfully downloaded {len(raw_transcript_filepaths)} raw transcript files.")
        else:
             print("No raw transcripts were downloaded successfully for the specified date range.")
    else:
        print("Download datetime range not specified in config/settings.yaml or is incomplete. Skipping download step.")
        # Si se salta la descarga, asumimos que los archivos JSON ya están en data/raw_transcripts/
        print(f"Assuming raw transcripts (JSON) are already in {data_paths['raw_transcripts']}.")
        raw_transcript_filepaths = glob.glob(os.path.join(data_paths["raw_transcripts"], "*.json"))
        print(f"Found {len(raw_transcript_filepaths)} existing raw transcript files.")

    if not raw_transcript_filepaths:
         print("No raw transcript files available after attempted download or finding existing. Exiting pipeline.")
         report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
         return


    # 3. Filtrado de Transcripciones Descargadas (por Wrap-up Code y extracción de IDs/Type)
    print("\n--- Step 2: Filtering Raw Transcripts and Extracting Initial Metadata ---") # Renumeramos
    # El filtro ahora devuelve una lista de diccionarios {'conversationId', 'communicationId', 'filepath', 'call_type'}
    filtered_calls_info = transcript_filter.filter_transcriptions(raw_transcript_filepaths)

    if not filtered_calls_info:
        print("No transcripts passed the filter or none were included (if filter disabled). Exiting pipeline.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
        return

    # 4. Generación de Metadatos de K+ (Consulta a Base de Datos)
    print("\n--- Step 3: K+ Metadata Generation (Database Lookup) ---") # Renumeramos
    # Inicializar el generador de metadatos
    metadata_generator = KplusMetadataGenerator()

    # Llamar a la función para generar y guardar los archivos JSON de metadatos
    # Esta función retorna la lista de call_info para los que se pudo generar el archivo de metadatos
    calls_with_metadata_info = metadata_generator.generate_kplus_metadata_files(filtered_calls_info)

    if not calls_with_metadata_info:
         print("No calls had K+ metadata successfully generated. Exiting pipeline.")
         report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
         return


    # 5. Procesamiento de Audio (Whisper) - Condicional
    # Este paso ahora toma las llamadas que tienen metadatos Y pasaron el filtro.
    print("\n--- Step 4: Audio Processing (Whisper) - Conditional ---") # Renumeramos
    use_whisper = pipeline_config.get("use_whisper", False)

    # Lista de diccionarios {IDs, filepath, call_type} para las transcripciones que se usarán para la evaluación
    # Por defecto, usamos las raw filtradas (que también tienen metadatos generados)
    transcriptions_for_evaluation_info = calls_with_metadata_info # Inicializamos con las que tienen metadatos


    if use_whisper:
         print("Whisper processing is enabled. Attempting to process audio for calls with generated metadata.")
         # Lógica futura:
         # raw_calls_dir = data_paths["raw_calls"]
         # # Obtener lista de call_info para las cuales se debe intentar procesar audio
         # # Esto se basa en calls_with_metadata_info
         # # Necesitarías descargar audios en Step 1 o antes, y nombrarlos consistentemente (ej. [conversationId].wav)
         # audio_files_to_process_info = [] # { 'conversationId', 'communicationId', 'audio_filepath' }
         # for call_info in calls_with_metadata_info:
         #      conv_id = call_info.get('conversationId')
         #      comm_id = call_info.get('communicationId')
         #      audio_path = os.path.join(raw_calls_dir, f"{conv_id}.wav") # Asume .wav
         #      if os.path.exists(audio_path):
         #           audio_files_to_process_info.append({'conversationId': conv_id, 'communicationId': comm_id, 'audio_filepath': audio_path})
         #      else:
         #           print(f"Warning: No audio file found for conversation {conv_id} at {audio_path}. Cannot process with Whisper.")
         #
         # if audio_files_to_process_info:
         #      # Procesar los audios existentes
         #      processed_whisper_paths_map = whisper_processor.process_audio_batch(audio_files_to_process_info) # Asumir batch retorna map {conv_id: whisper_txt_path}
         #
         #      # Actualizar la lista info_para_evaluacion para que apunte a Whisper si se procesó
         #      transcriptions_for_evaluation_info = []
         #      for call_info in calls_with_metadata_info: # Iterar sobre las que tienen metadatos
         #           conv_id = call_info['conversationId']
         #           whisper_path = processed_whisper_paths_map.get(conv_id)
         #           if whisper_path:
         #                # Usar la ruta de Whisper y marcar que se usó Whisper
         #                transcriptions_for_evaluation_info.append({
         #                     'conversationId': conv_id,
         #                     'communicationId': call_info['communicationId'],
         #                     'filepath': call_info['filepath'], # Mantener ruta al JSON raw (necesaria en data_loader)
         #                     'whisper_filepath': whisper_path, # Ruta al .txt de Whisper
         #                     'used_whisper': True
         #                })
         #           else:
         #                # Si no hay audio o Whisper falló para este call_id, usar la raw filtrada
         #                transcriptions_for_evaluation_info.append({
         #                     'conversationId': conv_id,
         #                     'communicationId': call_info['communicationId'],
         #                     'filepath': call_info['filepath'], # Ruta al archivo .json raw
         #                     'used_whisper': False # Aunque use_whisper es True en config, no se usó para esta llamada
         #                })
         #
         #      print(f"Whisper processing completed. Using {len([c for c in transcriptions_for_evaluation_info if c.get('used_whisper')])} Whisper transcripts and {len([c for c in transcriptions_for_evaluation_info if not c.get('used_whisper', True)])} raw filtered transcripts for evaluation.")
         #
         # else:
         #      print("No corresponding audio files found for calls with generated metadata to process with Whisper. Using raw filtered transcripts for all.")
         #      # transcriptions_for_evaluation_info ya es calls_with_metadata_info, y used_whisper=False por defecto
         #      for call_info in transcriptions_for_evaluation_info: call_info['used_whisper'] = False # Asegurar el flag si no hubo audio

    else:
        print("\n--- Step 4: Audio Processing (Skipped as per configuration) ---") # Renumeramos
        print(f"Using raw filtered transcripts from {data_paths['raw_transcripts']} for evaluation.")
        # Asegurar que la info_para_evaluacion tenga el flag use_whisper=False
        for call_info in transcriptions_for_evaluation_info:
             call_info['used_whisper'] = False


    # 6. Carga de Datos para Evaluación
    print(f"\n--- Step 5: Data Loading for Evaluation ({len(transcriptions_for_evaluation_info)} calls) ---") # Renumeramos

    all_evaluation_results_flat = []

    if not transcriptions_for_evaluation_info:
        print("No calls identified for evaluation after metadata generation and optional Whisper step. Exiting.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
        return

    print(f"Identified {len(transcriptions_for_evaluation_info)} calls to process for evaluation.")
    for call_info in transcriptions_for_evaluation_info:
        call_id = call_info['conversationId']
        raw_transcript_filepath = call_info['filepath']
        use_whisper_for_this_call = call_info.get('used_whisper', False)
        # Si se usó Whisper, data_loader necesita la ruta al .txt de Whisper
        # Añadir la ruta del archivo Whisper al dict call_info si existe
        # call_info['whisper_filepath'] = ... # Esto lo debería añadir el paso 4 si use_whisper es True

        # Cargar datos para la llamada actual
        # data_loader.load_call_data_for_evaluation ahora toma la RUTA al JSON raw y el flag use_whisper
        # Internamente, data_loader usará el flag use_whisper para decidir si carga .txt o parsea el JSON raw
        # Y CARGARÁ el JSON de metadatos generado por el Step 3 basado en call_id.
        call_data = data_loader.load_call_data_for_evaluation(
            call_id, # conversationId
            raw_transcript_filepath, # Ruta al JSON raw (necesaria para metadata y fallback text)
            use_whisper=use_whisper_for_this_call # Flag si se usará Whisper para el texto
            # Si Step 4 añadió 'whisper_filepath' a call_info, data_loader debería usarlo
            # call_info.get('whisper_filepath')
        )


        if not call_data or not call_data.get("transcript"):
            print(f"Skipping evaluation for call {call_id} due to data loading error or missing transcript content.")
            all_item_ids = [id for group in EVALUATION_STRUCTURE.values() for id in group['item_ids']]
            error_results = [{"call_id": call_id, "conversationId": call_info.get('conversationId'), "communicationId": call_info.get('communicationId'), "item_id": item_id, "result": "Error", "reason": "Failed to load transcript or data.", "transcript_segment": ""} for item_id in all_item_ids]
            all_evaluation_results_flat.extend(error_results)
            continue

        # 7. Evaluación con LLM
        print(f"\n--- Step 6: Evaluation for call {call_id} ---") # Renumeramos
        initial_results_for_call = []
        eval_structure_keys = pipeline_config.get("items_to_evaluate")
        if not eval_structure_keys:
            eval_structure_keys = list(EVALUATION_STRUCTURE.keys())
            print("No specific items_to_evaluate defined in config. Evaluating all defined groups/items.")
        else:
             valid_eval_keys = [k for k in eval_structure_keys if k in EVALUATION_STRUCTURE]
             if len(valid_eval_keys) != len(eval_structure_keys):
                  print(f"Warning: Some evaluation keys from config were not found in EVALUATION_STRUCTURE: {set(eval_structure_keys) - set(valid_eval_keys)}")
             eval_structure_keys = valid_eval_keys


        for evaluation_key in eval_structure_keys:
             item_ids_in_group = EVALUATION_STRUCTURE[evaluation_key]["item_ids"]
             print(f"  Evaluating group/item: {evaluation_key} ({item_ids_in_group}) for call {call_id}")

             group_results, input_t, output_t = evaluate_with_llm(evaluation_key, call_data['transcript'], call_data['call_metadata'])

             # Acumular conteos de tokens
             total_input_tokens += input_t
             total_output_tokens += output_t
             if evaluation_key in token_counts_by_eval_key:
                  token_counts_by_eval_key[evaluation_key]["input"] += input_t
                  token_counts_by_eval_key[evaluation_key]["output"] += output_t
             else:
                  token_counts_by_eval_key[evaluation_key] = {"input": input_t, "output": output_t}


             initial_results_for_call.extend(group_results)


        # 8. Post-Procesamiento
        print(f"\n--- Step 7: Post-Processing for call {call_id} ---") # Renumeramos
        final_results_for_call = apply_post_processing(initial_results_for_call, call_data['call_metadata'])

        # Añadir conversationId y communicationId a cada resultado individual
        conversation_id_for_results = call_data['call_metadata'].get('conversationId', call_id)
        communication_id_for_results = call_data['call_metadata'].get('communicationId', call_id)

        for res in final_results_for_call:
            if 'call_id' not in res:
                 res['call_id'] = call_id
            res['conversationId'] = conversation_id_for_results
            res['communicationId'] = communication_id_for_results

        all_evaluation_results_flat.extend(final_results_for_call)


    # 9. Guardar Resultados
    print("\n--- Step 8: Saving Results ---") # Renumeramos
    if all_evaluation_results_flat:
        results_handler.save_evaluation_results_to_json(all_evaluation_results_flat, "evaluation_output.json")
    else:
        print("No evaluation results generated to save.")

    # 10. Reportar Conteo de Tokens
    print("\n--- LLM Token Usage Summary ---") # Renumeramos
    report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)

    print("\n--- Automated Call Evaluation Pipeline Finished ---")


def report_token_counts(total_input: int, total_output: int, counts_by_key: dict):
    """Imprime un resumen de los tokens utilizados."""
    print(f"Total Input Tokens: {total_input}")
    print(f"Total Output Tokens: {total_output}")
    print("\nTokens by Evaluation Key:")
    for key, counts in counts_by_key.items():
        print(f"  - {key}: Input = {counts['input']}, Output = {counts['output']}")
    print("\nNote: Token counts are approximate and depend on the specific model tokenizer.")
    print("Refer to Azure OpenAI pricing for your specific model deployment to estimate cost.")


# Punto de entrada del script
if __name__ == "__main__":
    main()


11. Actualizar src/data_preparation/data_loader.py (para cargar metadatos generados)
Modificamos load_call_data_for_evaluation para cargar el archivo JSON de metadatos desde data/k_plus_data/[call_id].json en lugar de generarlo dummy.

import os
import json
from datetime import datetime, timedelta
from pytz import timezone # Importar timezone

from src.config import config_manager

# Logger específico para este módulo
import logging
data_loader_logger = logging.getLogger(__name__)
# Configuración básica si no existe
if not data_loader_logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


def extract_formatted_transcript_text(transcript_json_data):
    """
    Extrae y formatea el texto de la transcripción de un JSON raw de Genesys,
    intercalando los hablantes.
    """
    frases_formateadas = []
    hablante_anterior = None

    transcripts_array = transcript_json_data.get("transcripts", [])
    for transcript_entry in transcripts_array:
        if 'VOICE_TRANSCRIPTION' in transcript_entry.get('features', []):
            phrases = transcript_entry.get('phrases', [])
            for phrase in phrases:
                texto = phrase.get("text", "").strip()
                purpose = phrase.get("participantPurpose", "")

                if purpose == "internal":
                    hablante = "Agente"
                elif purpose == "external":
                    hablante = "Interlocutor"
                else:
                    hablante = "Desconocido"

                if not texto:
                    continue

                if frases_formateadas and hablante == hablante_anterior:
                    frases_formateadas[-1] += f" {texto}"
                else:
                    frases_formateadas.append(f"{hablante}: {texto}")
                    hablante_anterior = hablante

    return "\n".join(frases_formateadas)


def load_call_data_for_evaluation(call_id: str, raw_transcript_filepath: str, use_whisper: bool = True, whisper_filepath: str = None):
    """
    Carga y prepara todos los datos necesarios para la evaluación de una sola llamada.

    Args:
        call_id (str): El ID único de la llamada (conversationId).
        raw_transcript_filepath (str): Ruta al archivo JSON de la transcripción raw descargada.
        use_whisper (bool): Si intentar usar la transcripción de Whisper.
        whisper_filepath (str, optional): Ruta explícita al archivo .txt de Whisper si use_whisper es True.
                                          Si es None, se construirá la ruta estándar.

    Returns:
        dict: Diccionario `call_data` con 'transcript' (texto plano) y 'call_metadata',
              o None si los archivos necesarios no se encuentran o fallan al cargar.
    """
    data_loader_logger.info(f'Loading data for call ID: {call_id}')

    data_paths = config_manager.get("data_paths")
    whisper_transcript_dir = data_paths["whisper_transcripts"]
    k_plus_data_dir = data_paths["k_plus_data"] # Directorio donde están los JSON de metadatos generados

    raw_transcript_json = None
    transcript_content = None # El texto plano final a usar para evaluación

    # --- 1. Cargar y Parsear el JSON de Transcripción Raw (para metadata y fallback text) ---
    try:
        with open(raw_transcript_filepath, 'r', encoding='utf-8') as f:
            raw_transcript_json = json.load(f)
        data_loader_logger.info(f'Loaded raw transcript JSON from: {raw_transcript_filepath}')

        # Extraer el texto plano formateado del JSON raw
        transcript_content_raw_formatted = extract_formatted_transcript_text(raw_transcript_json)
        if not transcript_content_raw_formatted:
             data_loader_logger.warning(f'Extracted empty text content from raw transcript JSON for call {call_id}. File: {raw_transcript_filepath}')


    except FileNotFoundError:
        data_loader_logger.error(f'Raw transcript file not found for call {call_id} at {raw_transcript_filepath}. This should not happen if filter passed it.', {"call_id": call_id})
        return None
    except json.JSONDecodeError:
        data_loader_logger.error(f'Error decoding JSON for raw transcript: {raw_transcript_filepath}.', {"call_id": call_id})
        return None
    except Exception as e:
        data_loader_logger.error(f'An unexpected error occurred loading raw transcript {raw_transcript_filepath}: {e}', {"call_id": call_id})
        return None


    # --- 2. Intentar cargar Transcripción Whisper si aplica ---
    if use_whisper:
        # Usar la ruta proporcionada o construir la ruta estándar
        whisper_file_to_load = whisper_filepath if whisper_filepath else os.path.join(whisper_transcript_dir, f"{call_id}.txt")

        if whisper_file_to_load and os.path.exists(whisper_file_to_load):
            try:
                with open(whisper_file_to_load, 'r', encoding='utf-8') as f:
                    transcript_content_whisper = f.read()
                if transcript_content_whisper.strip(): # Usar la de Whisper si no está vacía
                    transcript_content = transcript_content_whisper.strip()
                    data_loader_logger.info(f'Loaded Whisper transcript from: {whisper_file_to_load}. Using for evaluation.')
                else:
                     # Si el archivo Whisper está vacío, usar la transcripción formateada del JSON raw
                     transcript_content = transcript_content_raw_formatted
                     data_loader_logger.warning(f'Whisper transcript file {whisper_file_to_load} is empty. Falling back to raw transcript text.')

            except Exception as e:
                 # Errores al cargar Whisper, usar la del JSON raw
                 transcript_content = transcript_content_raw_formatted
                 data_loader_logger.error(f'Error loading Whisper transcript {whisper_file_to_load}: {e}. Falling back to raw transcript text.', {"call_id": call_id})
        else:
             # Si no se encuentra el archivo Whisper, usar la transcripción del JSON raw
             transcript_content = transcript_content_raw_formatted
             data_loader_logger.warning(f'Whisper transcript file not found at {whisper_file_to_load} or path not provided. Falling back to raw transcript text.')
    else:
        # Si use_whisper es False, usamos directamente la transcripción formateada del JSON raw
        transcript_content = transcript_content_raw_formatted
        data_loader_logger.info('use_whisper is False. Using raw transcript text for evaluation.')

    # Si a pesar de todo la transcripción está vacía, no podemos evaluar
    if not transcript_content:
         data_loader_logger.error(f'Final transcript content is empty for call {call_id}. Cannot evaluate.')
         return None


    # --- 3. Cargar Metadatos Generados de K+ ---
    # Este archivo DEBE existir si el paso de generación de metadatos fue exitoso.
    metadata_filepath = os.path.join(k_plus_data_dir, f"{call_id}.json")
    k_plus_data_snapshot = None # Inicializar como None
    call_metadata = {} # Inicializar el diccionario de metadatos que se pasará al LLM

    try:
        with open(metadata_filepath, 'r', encoding='utf-8') as f:
            # El archivo JSON generado contiene directamente el snapshot de K+
            k_plus_data_snapshot = json.load(f)
            data_loader_logger.info(f'Loaded K+ metadata snapshot from: {metadata_filepath}')

    except FileNotFoundError:
         # Esto no debería pasar si el pipeline está correcto y el generador tuvo éxito.
         # Si pasa, significa que el generador falló para esta llamada o hubo un error de ruta.
         data_loader_logger.error(f"K+ metadata snapshot file NOT found for call {call_id} at {metadata_filepath}. This call was likely skipped in K+ metadata generation.", {"call_id": call_id})
         # No podemos continuar con la evaluación si los metadatos de K+ críticos faltan
         return None
    except json.JSONDecodeError:
         data_loader_logger.error(f"Error decoding JSON from K+ metadata snapshot file {metadata_filepath} for call {call_id}. Cannot proceed.", {"call_id": call_id})
         return None
    except Exception as e:
         data_loader_logger.error(f'An unexpected error occurred loading K+ metadata snapshot for call {call_id}: {e}. Cannot proceed.', {"call_id": call_id})
         return None


    # --- 4. Ensamblar call_metadata final fusionando info del JSON Raw y K+ Snapshot ---
    # Empezamos con los datos extraídos del JSON raw (IDs, tipo llamada, wrap-up)
    # NOTA: Esta info (IDs, tipo llamada, wrap-up) también está en el K+ metadata snapshot si el generador la puso.
    # Se puede decidir si tomarla del JSON raw original (más fiable para la transcripción)
    # o del snapshot generado (más fiable si el generador corrige/estandariza).
    # Para la POC, vamos a tomar IDs, tipo, wrap-up del JSON raw, y el resto de k_plus_data_snapshot.
    # Esto es lo que espera la lógica del evaluador y post-procesador actualmente.

    call_metadata['call_id'] = raw_transcript_json.get('conversationId', call_id) # ConversationId
    call_metadata['conversationId'] = call_metadata['call_id']
    call_metadata['communicationId'] = raw_transcript_json.get('communicationId', call_id)

    # Extraer call_type del JSON raw
    call_type_raw = raw_transcript_json.get('participants', [{}])[0].get('initialDirection', 'unknown')
    if isinstance(call_type_raw, str):
         call_metadata['call_type'] = 'saliente' if call_type_raw.lower() == 'outbound' else 'entrante' if call_type_raw.lower() == 'inbound' else 'Desconocido'
    else:
         call_metadata['call_type'] = 'Desconocido'

    # Extraer wrap-up info del JSON raw y usar el mapeo
    wrap_up_code_raw = None
    participants = raw_transcript_json.get("participants", [])
    for participant in participants:
         if participant.get("participantPurpose") == "agent":
              wrap_up_code_raw = participant.get("wrapUpCode")
              break
    call_metadata['wrap_up_code'] = wrap_up_code_raw
    wrapup_codes_mapping = config_manager.get_ref_data("wrapup_codes")
    call_metadata['wrap_up_info'] = wrapup_codes_mapping.get(wrap_up_code_raw, {}) if wrapup_codes_mapping and wrap_up_code_raw else {}


    # Extraer otros metadatos útiles del JSON raw
    call_metadata['raw_duration_ms'] = raw_transcript_json.get('duration', {}).get('milliseconds')
    call_metadata['raw_start_time_ms'] = raw_transcript_json.get('startTime')
    call_metadata['conversationStartTime'] = raw_transcript_json.get('conversationStartTime')
    call_metadata['conversationDuration'] = raw_transcript_json.get('conversationDuration')
    call_metadata['duration_minutes'] = call_metadata['conversationDuration'] / 60000.0 if isinstance(call_metadata['conversationDuration'], (int, float)) else 0


    # Añadir el snapshot de datos de K+ (cargado)
    call_metadata['k_plus_data_snapshot'] = k_plus_data_snapshot if k_plus_data_snapshot is not None else {}

    # Añadir la ruta de la transcripción cargada y si se usó Whisper para referencia
    call_metadata['raw_transcript_filepath'] = raw_transcript_filepath
    call_metadata['used_whisper_transcript'] = use_whisper
    if use_whisper and whisper_filepath:
        call_metadata['whisper_transcript_filepath'] = whisper_filepath


    # Añadir otros flags o metadatos que puedan venir de la BD pero no estén en el snapshot si tu generador los pone a nivel principal
    # Ejemplo: si el generador añade 'is_authorised_express' al JSON que guarda, lo f fusionas aquí.
    # Asumimos que el generador solo guarda el snapshot, por lo que estos campos deben venir de otra fuente
    # o ser añadidos con defaults si no están en el snapshot y el prompt los espera.
    # Es mejor si el generador los añade al snapshot si los obtiene de la BD.
    # Ejemplo: k_plus_data_snapshot['is_authorised_express'] = ...
    # Entonces el prompt_data builder los tomaría de ahí.
    # Si el generador no los pone ahí, tendrías que añadirlos aquí con defaults:
    # call_metadata['is_authorised_express'] = k_plus_data_snapshot.get('is_authorised_express', False)


    # Asegurar que flags/datos calculados necesarios para prompts estén presentes si no vienen del snapshot
    # is_prescription_flag_recent se calcula en el generador de metadatos ahora
    # call_metadata["is_prescription_flag_recent"] = call_metadata.get("is_prescription_flag_recent", False) # No, ahora viene del snapshot
    # Los campos que el prompt espera en prompt_data dict deben estar o en call_metadata (directamente o via keys de raw JSON)
    # o dentro de call_metadata['k_plus_data_snapshot'].
    # El prompt_data builder en evaluator.py necesita adaptarse a la estructura FINAL de call_metadata
    # y call_metadata['k_plus_data_snapshot'] que produce este data_loader.


    data_loader_logger.info(f'Data prepared for call {call_id}.')
    return call_data

12. Actualizar src/main_workflow.py (Conectar el flujo completo)
Ajustamos el flujo para incluir el nuevo paso de generación de metadatos.

import os
import glob
from datetime import datetime, timedelta
from pytz import timezone
import pytz

from src.config import config_manager
from src.data_acquisition import downloader
from src.audio_processing import whisper_processor
from src.data_preparation import data_loader, transcript_filter
from src.evaluation import evaluate_with_llm, apply_post_processing, EVALUATION_STRUCTURE
from src.results import results_handler
# Importar el nuevo generador de metadatos
from src.database import KplusMetadataGenerator


def main():
    """
    Función principal que ejecuta el pipeline de evaluación.
    """
    print("--- Starting Automated Call Evaluation Pipeline ---")

    # 1. Cargar Configuración
    config = config_manager

    data_paths = config.get("data_paths")
    pipeline_config = config.get("pipeline")
    genesys_config = config.get("genesys_cloud")
    db_config = config.get("database")

    if not all([data_paths, pipeline_config, genesys_config, db_config]):
        print("Error: Failed to load essential configuration (data_paths, pipeline, genesys_cloud, or database). Exiting.")
        return

    # Asegurar directorios existen
    for path_key in data_paths.values():
        if isinstance(path_key, str):
             os.makedirs(path_key, exist_ok=True)

    # --- Inicializar contadores globales de tokens ---
    total_input_tokens = 0
    total_output_tokens = 0
    token_counts_by_eval_key = {key: {"input": 0, "output": 0} for key in EVALUATION_STRUCTURE.keys()}


    # 2. Adquisición de Datos (Genesys Transcriptions Download)
    print("\n--- Step 1: Data Acquisition (Genesys Transcriptions Download) ---")
    download_date_range_config = pipeline_config.get("download_date_range")

    raw_transcript_filepaths = [] # Inicializar siempre la lista

    if download_date_range_config and download_date_range_config.get("start_datetime") and download_date_range_config.get("end_datetime"):
        start_datetime_str = download_date_range_config["start_datetime"]
        end_datetime_str = download_date_range_config["end_datetime"]

        try:
             raw_transcript_filepaths = downloader.download_transcriptions_batch(start_datetime_str, end_datetime_str)

        except Exception as e:
             print(f"An error occurred during the download step: {e}")

        if raw_transcript_filepaths:
             print(f"Successfully downloaded {len(raw_transcript_filepaths)} raw transcript files.")
        else:
             print("No raw transcripts were downloaded successfully for the specified date range.")
    else:
        print("Download datetime range not specified in config/settings.yaml or is incomplete. Skipping download step.")
        print(f"Assuming raw transcripts (JSON) are already in {data_paths['raw_transcripts']}.")
        raw_transcript_filepaths = glob.glob(os.path.join(data_paths["raw_transcripts"], "*.json"))
        print(f"Found {len(raw_transcript_filepaths)} existing raw transcript files.")

    if not raw_transcript_filepaths:
         print("No raw transcript files available after attempted download or finding existing. Exiting pipeline.")
         report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
         return


    # 3. Filtrado de Transcripciones Descargadas (por Wrap-up Code y extracción de IDs/Type)
    print("\n--- Step 2: Filtering Raw Transcripts and Extracting Initial Metadata ---")
    # El filtro ahora devuelve una lista de diccionarios {'conversationId', 'communicationId', 'filepath', 'call_type'}
    filtered_calls_info = transcript_filter.filter_transcriptions(raw_transcript_filepaths)

    if not filtered_calls_info:
        print("No transcripts passed the filter or none were included (if filter disabled). Exiting pipeline.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
        return


    # 4. Generación de Metadatos de K+ (Consulta a Base de Datos)
    print("\n--- Step 3: K+ Metadata Generation (Database Lookup) ---")
    # Inicializar el generador de metadatos
    metadata_generator = KplusMetadataGenerator()

    # Llamar a la función para generar y guardar los archivos JSON de metadatos
    # Esta función retorna la lista de call_info para los que se pudo generar el archivo de metadatos
    calls_with_metadata_info = metadata_generator.generate_kplus_metadata_files(filtered_calls_info)

    if not calls_with_metadata_info:
         print("No calls had K+ metadata successfully generated. Exiting pipeline.")
         report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
         return


    # 5. Procesamiento de Audio (Whisper) - Condicional
    # Este paso ahora toma las llamadas que tienen metadatos Y pasaron el filtro.
    print("\n--- Step 4: Audio Processing (Whisper) - Conditional ---")
    use_whisper = pipeline_config.get("use_whisper", False)

    transcriptions_for_evaluation_info = calls_with_metadata_info # Inicializamos con las que tienen metadatos

    if use_whisper:
         print("Whisper processing is enabled. Attempting to process audio for calls with generated metadata.")
         # Lógica futura (ver implementación anterior) para actualizar transcriptions_for_evaluation_info
         # para que apunte a los archivos .txt de Whisper y marque 'used_whisper': True
         # Y añadir 'whisper_filepath' al diccionario de call_info si se procesó con éxito.


    else:
        print("\n--- Step 4: Audio Processing (Skipped as per configuration) ---")
        print(f"Using raw filtered transcripts from {data_paths['raw_transcripts']} for evaluation.")
        for call_info in transcriptions_for_evaluation_info:
             call_info['used_whisper'] = False


    # 6. Carga de Datos para Evaluación
    print(f"\n--- Step 5: Data Loading for Evaluation ({len(transcriptions_for_evaluation_info)} calls) ---")
    data_loader_logger = logging.getLogger("src.data_preparation.data_loader") # Obtener el logger del data_loader para usarlo aquí si es necesario


    all_evaluation_results_flat = []


    if not transcriptions_for_evaluation_info:
        print("No calls identified for evaluation after metadata generation and optional Whisper step. Exiting.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
        return


    print(f"Identified {len(transcriptions_for_evaluation_info)} calls to process for evaluation.")
    for call_info in transcriptions_for_evaluation_info:
        call_id = call_info['conversationId']
        raw_transcript_filepath = call_info['filepath'] # Ruta al JSON raw (necesaria en data_loader)
        use_whisper_for_this_call = call_info.get('used_whisper', False)
        whisper_filepath_for_call = call_info.get('whisper_filepath') # Ruta al .txt de Whisper si existe


        # Cargar datos para la llamada actual
        call_data = data_loader.load_call_data_for_evaluation(
            call_id, # conversationId
            raw_transcript_filepath, # Ruta al JSON raw
            use_whisper=use_whisper_for_this_call, # Flag si se usará Whisper para el texto
            whisper_filepath=whisper_filepath_for_call # Ruta al .txt de Whisper si se usó
        )


        if not call_data or not call_data.get("transcript"):
            print(f"Skipping evaluation for call {call_id} due to data loading error or missing transcript content.")
            all_item_ids = [id for group in EVALUATION_STRUCTURE.values() for id in group['item_ids']]
            error_results = [{"call_id": call_id, "conversationId": call_info.get('conversationId'), "communicationId": call_info.get('communicationId'), "item_id": item_id, "result": "Error", "reason": "Failed to load transcript or data.", "transcript_segment": ""} for item_id in all_item_ids]
            all_evaluation_results_flat.extend(error_results)
            continue

        # 7. Evaluación con LLM
        print(f"\n--- Step 6: Evaluation for call {call_id} ---")
        initial_results_for_call = []
        eval_structure_keys = pipeline_config.get("items_to_evaluate")
        if not eval_structure_keys:
            eval_structure_keys = list(EVALUATION_STRUCTURE.keys())
            print("No specific items_to_evaluate defined in config. Evaluating all defined groups/items.")
        else:
             valid_eval_keys = [k for k in eval_structure_keys if k in EVALUATION_STRUCTURE]
             if len(valid_eval_keys) != len(eval_structure_keys):
                  print(f"Warning: Some evaluation keys from config were not found in EVALUATION_STRUCTURE: {set(eval_structure_keys) - set(valid_eval_keys)}")
             eval_structure_keys = valid_eval_keys


        for evaluation_key in eval_structure_keys:
             item_ids_in_group = EVALUATION_STRUCTURE[evaluation_key]["item_ids"]
             print(f"  Evaluating group/item: {evaluation_key} ({item_ids_in_group}) for call {call_id}")

             # evaluate_with_llm retorna resultados, input_tokens, output_tokens
             group_results, input_t, output_t = evaluate_with_llm(evaluation_key, call_data['transcript'], call_data['call_metadata'])

             # Acumular conteos de tokens
             total_input_tokens += input_t
             total_output_tokens += output_t
             if evaluation_key in token_counts_by_eval_key:
                  token_counts_by_eval_key[evaluation_key]["input"] += input_t
                  token_counts_by_eval_key[evaluation_key]["output"] += output_t
             else:
                  token_counts_by_eval_key[evaluation_key] = {"input": input_t, "output": output_t}


             initial_results_for_call.extend(group_results)


        # 8. Post-Procesamiento
        print(f"\n--- Step 7: Post-Processing for call {call_id} ---")
        final_results_for_call = apply_post_processing(initial_results_for_call, call_data['call_metadata'])

        # Añadir conversationId y communicationId a cada resultado individual
        conversation_id_for_results = call_data['call_metadata'].get('conversationId', call_id)
        communication_id_for_results = call_data['call_metadata'].get('communicationId', call_id)

        for res in final_results_for_call:
            if 'call_id' not in res:
                 res['call_id'] = call_id
            res['conversationId'] = conversation_id_for_results
            res['communicationId'] = communication_id_for_results

        all_evaluation_results_flat.extend(final_results_for_call)


    # 9. Guardar Resultados
    print("\n--- Step 8: Saving Results ---")
    if all_evaluation_results_flat:
        results_handler.save_evaluation_results_to_json(all_evaluation_results_flat, "evaluation_output.json")
    else:
        print("No evaluation results generated to save.")

    # 10. Reportar Conteo de Tokens
    print("\n--- LLM Token Usage Summary ---")
    report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)

    print("\n--- Automated Call Evaluation Pipeline Finished ---")


def report_token_counts(total_input: int, total_output: int, counts_by_key: dict):
    """Imprime un resumen de los tokens utilizados."""
    print(f"Total Input Tokens: {total_input}")
    print(f"Total Output Tokens: {total_output}")
    print("\nTokens by Evaluation Key:")
    for key, counts in counts_by_key.items():
        print(f"  - {key}: Input = {counts['input']}, Output = {counts['output']}")
    print("\nNote: Token counts are approximate and depend on the specific model tokenizer.")
    print("Refer to Azure OpenAI pricing for your specific model deployment to estimate cost.")


# Punto de entrada del script
if __name__ == "__main__":
    main()

Con estos cambios, el pipeline ahora incluye la generación de metadatos de K+ desde la base de datos para las llamadas que pasaron el filtro.
Pasos a Seguir:
Instala oracledb y pandas.
Rellena tu .env con DB_USER y DB_PASSWORD.
Rellena config/settings.yaml con los detalles de conexión a la BD y los nombres de las tablas.
Crucial: Implementa el resto de la lógica de consulta en _get_account_and_financial_details en src/database/kplus_metadata_generator.py para obtener la información financiera (Importe Deuda, Cedente, Producto, Flags de Prescripción) siguiendo la cadena de tablas que describiste.
Adapta los rules_detail en src/evaluation/item_rules.py: Asegúrate de que los prompts referencien correctamente la nueva estructura de datos dentro de k_plus_data_snapshot generada por KplusMetadataGenerator. Por ejemplo, si el importe de deuda está ahora en k_plus_data_snapshot.importe_deuda, las reglas del Ítem 5 deben usar esa clave.
Ejecuta el pipeline (python scripts/run_pipeline.py).
Ahora el pipeline: Descarga -> Filtra -> Consulta BD y Genera Metadatos -> Opcional Whisper -> Carga (transcripción + metadatos DB) -> Evalúa LLM -> Post-procesa -> Guarda Resultados + Reporte de Tokens.



Tienes toda la razón, fue un descuido. Para que el LLM pueda utilizar la rica información extraída de la base de datos de Kollecto, es indispensable actualizar los prompts en src/evaluation/item_rules.py para que hagan referencia explícita a la nueva estructura de k_plus_data_snapshot.
Vamos a actualizar el contenido de src/evaluation/item_rules.py, adaptando las descripciones (rules_detail) de los ítems que usan datos de K+ (principalmente 4, 5, 6, 7, 20) para guiar al modelo sobre cómo interpretar la información que ahora se le proporciona en el snapshot.
Primero, haremos una pequeña mejora en src/database/kplus_metadata_generator.py para añadir los conteos totales de teléfonos y emails al snapshot, lo cual es útil para la regla del Ítem 7.
Actualización menor en src/database/kplus_metadata_generator.py
Dentro del método _get_subscriber_contacts, después de obtener las listas phones_valid_0_2 y emails_valid_0_2, añadiremos los conteos al diccionario refined_contacts.
    def _get_subscriber_contacts(self, subscriber_id):
        """
        Obtiene direcciones, teléfonos y emails válidos según criterios (VALID, LAST_UPDATE_DATE, SOURCE).
        Incluye listas separadas para diferentes validaciones y añade conteos.
        """
        contacts = {
            'addresses_valid_0_2': [],
            'phones_valid_0_2': [],
            'emails_valid_0_2': [],
            'phones_valid_6_source_82': [] # Para Item 20 específico
        }
        conn = self._get_connection()
        if conn is None:
            return contacts

        # Criterios comunes de filtro
        valid_codes_0_2 = (0, 2)
        valid_code_2 = (2,) # Solo para Ítem 20 base (será filtrado de 0_2 listas)
        valid_code_6 = (6,) # Solo para Ítem 20 específico
        source_code_80 = 80
        source_code_82 = 82
        # Obtener la fecha de referencia para < 6 meses
        six_months_ago = datetime.now() - timedelta(days=180)


        # --- Direcciones (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_addresses = f"""
        SELECT STREET, CITY, DISTRICT, POSTAL_CODE, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_addresses')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago -- Filtrar por fecha > 6 meses atrás
          AND SOURCE = :source_code
        """
        df_addresses = self._query_table(query_addresses, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_addresses.iterrows():
            address_parts = [row.get('STREET'), row.get('CITY'), row.get('DISTRICT'), row.get('POSTAL_CODE')]
            formatted_address = ", ".join([part for part in address_parts if pd.notna(part)])
            if formatted_address:
                contacts['addresses_valid_0_2'].append({
                    'address': formatted_address,
                    'valid_code': row.get('VALID')
                })

        db_gen_logger.debug(f"Found {len(contacts['addresses_valid_0_2'])} valid addresses (0,2) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Teléfonos (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_phones_0_2 = f"""
        SELECT PHONE_NUMBER, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_phones')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago -- Filtrar por fecha > 6 meses atrás
          AND SOURCE = :source_code
        """
        df_phones_0_2 = self._query_table(query_phones_0_2, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_phones_0_2.iterrows():
             phone = row.get('PHONE_NUMBER')
             if pd.notna(phone):
                  contacts['phones_valid_0_2'].append({
                      'phone_number': str(phone).strip(),
                      'valid_code': row.get('VALID')
                  })

        db_gen_logger.debug(f"Found {len(contacts['phones_valid_0_2'])} valid phones (0,2) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Emails (VALID 0 o 2, LastUpdate < 6m, Source 80) ---
        query_emails_0_2 = f"""
        SELECT EMAIL, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_emails')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND LAST_UPDATE_DATE >= :six_months_ago -- Filtrar por fecha > 6 meses atrás
          AND SOURCE = :source_code
        """
        df_emails_0_2 = self._query_table(query_emails_0_2, params={'subscriber_id': subscriber_id, 'valid_codes': valid_codes_0_2, 'six_months_ago': six_months_ago, 'source_code': source_code_80})

        for index, row in df_emails_0_2.iterrows():
             email = row.get('EMAIL')
             if pd.notna(email):
                  contacts['emails_valid_0_2'].append({
                      'email': str(email).strip(),
                      'valid_code': row.get('VALID')
                  })
        db_gen_logger.debug(f"Found {len(contacts['emails_valid_0_2'])} valid emails (0,2) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Item 20 Specific Phones (VALID 6, Source 82) ---
        query_phones_valid_6 = f"""
        SELECT PHONE_NUMBER, VALID, LAST_UPDATE_DATE, SOURCE
        FROM {self.table_names.get('subscriber_phones')}
        WHERE SUBSCRIBER_ID = :subscriber_id
          AND VALID IN :valid_codes
          AND SOURCE = :source_code
        """
        df_phones_valid_6 = self._query_table(query_phones_valid_6, params={'subscriber_id': subscriber_id, 'valid_codes': valid_code_6, 'source_code': source_code_82})

        for index, row in df_phones_valid_6.iterrows():
             phone = row.get('PHONE_NUMBER')
             if pd.notna(phone):
                  contacts['phones_valid_6_source_82'].append({
                      'phone_number': str(phone).strip(),
                      'valid_code': row.get('VALID'), # Será 6
                      'source_code': row.get('SOURCE') # Será 82
                  })
        db_gen_logger.debug(f"Found {len(contacts['phones_valid_6_source_82'])} phones (VALID 6, SOURCE 82) for SUBSCRIBER_ID: {subscriber_id}")


        # --- Refined contact structure including VALID=2 specifics for Item 20 base and counts for Item 7 ---
        refined_contacts = {
            'addresses_valid_0_2': contacts['addresses_valid_0_2'],
            'phones_valid_0_2': contacts['phones_valid_0_2'],
            'emails_valid_0_2': contacts['emails_valid_0_2'],
            'phones_valid_6_source_82': contacts['phones_valid_6_source_82'],

             # Campos específicos para Item 20 base (VALID=2) - extraídos de los 0_2 listas
             'addresses_valid_2': [addr for addr in contacts['addresses_valid_0_2'] if addr.get('valid_code') == 2],
             'phones_valid_2': [ph for ph in contacts['phones_valid_0_2'] if ph.get('valid_code') == 2],
             'emails_valid_2': [em for em in contacts['emails_valid_0_2'] if em.get('valid_code') == 2],

             # Conteo total de contactos válidos (0 o 2) para Item 7
             'numero_telefonos_total_validos': len(contacts['phones_valid_0_2']),
             'numero_emails_total_validos': len(contacts['emails_valid_0_2']),
        }


        return refined_contacts

Ahora, actualicemos src/evaluation/item_rules.py para que los prompts usen estas nuevas claves en k_plus_data_snapshot.
Actualización de src/evaluation/item_rules.py
Modificaremos los rules_detail para los ítems 4, 5, 6, 7, 20 y el prompt_template para el grupo "inicio_llamada_group".
# Este archivo contiene la definición de las reglas de evaluación
# y la estructura que define cómo se agrupan los ítems para la evaluación LLM.

# Este archivo contiene la definición de las reglas de evaluación
# y la estructura que define cómo se agrupan los ítems para la evaluación LLM.

# --- Definición de Items y sus reglas (completo para los 10 ítems en scope) ---
# COPIA AQUÍ EL CONTENIDO DEL DICCIONARIO 'ITEM_RULES' DEL PASO ANTERIOR
# Asegúrate de que las reglas hagan referencia explícita a los campos
# dentro de 'k_plus_data_snapshot' cuando sea necesario.
ITEM_RULES = {
    # Bloque 1: Inicio Llamada
    "1": { "name": "Saludo+Identificación (agente+empresa)", "complexity": "BAJO", "description": "...",
            "rules_detail": """
            Se valora que el saludo y la identificación del agente sean las establecidas por la empresa.
            - Llamadas Salientes: Agente debe presentarse con Nombre + 1 Apellido e indicar que llama de EOS Spain.
            - Llamadas Entrantes: Ajustarse a frase “EOS Spain buenos días/tardes le atiende nombre +er apellido, ¿en qué puedo ayudarle?”
            - Excepciones y otras consideraciones: Si agente solo indica su nombre, se acepta. Si frase en diferente orden, se acepta.
            - Relación con otros ítems: Sin relación directa.
            """},
    "2": { "name": "Informar posible grabación llamada + GDPR", "complexity": "BAJO", "description": "...",
            "rules_detail": """
            Informar siguiendo argumentario en TODAS llamadas salientes (incl. callback). RedSys: informar parar/reanudar grabación. Añadir persona: identificarse e informar grabación.
            - Excepciones: Cliente interrumpe, no se valora (Pass). Frase no exacta pero informa, correcto (Pass). Empresas autorizadas con NDA, no es necesario informar (Pass/N/A si aplica).
            - Relación con otros ítems: Sin relación directa.
            """},
    "3": { "name": "Identificación interlocutor (buscar confirmación)", "complexity": "BAJO", "description": "...",
            "rules_detail": """
            Agente debe buscar confirmación de que habla con la persona correcta (cliente).
            - Llamadas IN: Solicitar interlocutor facilite nombre y motivo. Si es cliente, pedir Nombre Completo, DNI, Fecha Nacimiento. -> **Los datos de referencia son Nombre Completo, SSN, Birth Date de los suscriptores en k_plus_data_snapshot.subscribers.**
            - Llamadas OUT: Preguntar por interviniente con Nombre y Apellidos. Debe responder afirmativamente a "¿Es usted?". No basta con "Sí", "Dígame". -> **Verificar si el nombre mencionado en la transcripción corresponde a full_name de los suscriptores en k_plus_data_snapshot.subscribers.**
            - Excepciones: Interlocutor en llamada IN referencia su deuda ("llamo por mi tarjeta"), NO necesario el check "¿es usted?" (Pass). Llamada IN: agente pide datos ANTES de saber si es cliente, penaliza. Llamada IN: agente lee nombre SIN solicitar interlocutor lo facilite, penaliza. Penalizar solicitar 3 últimos dígitos DNI SIN negativa previa a confirmar completo (relacionado con Ítem 4, pero penaliza aquí).
            - Relación con otros ítems: Sin relación directa.
            """},
    "4": { "name": "Nombre completo Interviniente, DNI/NIF, Fecha nacimiento", "complexity": "MEDIO", "description": "...",
           "rules_detail": """
           Evalúa correcta identificación (Nombre Completo, DNI/NIE, Fecha Nacimiento) según tipo llamada.
           - Llamadas Salientes: Confirmar nombre completo Y facilitar DNI/NIE completo.
           - Llamadas Entrantes: Confirmar nombre completo + DNI/NIE completo + fecha nacimiento.
           - Los datos de referencia son los detalles de los suscriptores disponibles en **k_plus_data_snapshot.subscribers** (lista de personas asociadas al Subscriber ID principal). Cada entrada en esta lista puede tener 'full_name', 'ssn', 'birth_date'.
           - Excepciones:
             - NO informar motivo antes identificar.
             - Solo solicitar 3 últimos dígitos DNI/NIE si hay NEGATIVA previa. (La penalización por pedir 3 últimos SIN negativa previa va en Ítem 3). Aquí se valora si la excepción se aplicó correctamente DESPUÉS de una negativa.
             - Si **'birth_date' NO está presente o es null/vacío** en los datos del suscriptor en **k_plus_data_snapshot.subscribers** Y el agente valida por dirección (comparar con las direcciones en **k_plus_data_snapshot.addresses_valid_0_2**), se considera correcto.
             - Llamadas entrantes por SMS/perdida donde agente da nombre: Válido si sigue resto (pide DNI/fecha nacimiento).
             - Autorizados expresos: (Nueva lógica añadida) Para Autorizados expresos (**k_plus_data_snapshot.is_authorised_express** es true), se pide Nombre completo + SSN del Autorizado (no se verifica con BD). PERO se deben verificar los datos del Interviniente asociado (Nombre completo + SSN). -> **Verificar si el agente pide Nombre + SSN del autorizado Y si busca corroborar Nombre completo + SSN del interviniente principal usando k_plus_data_snapshot.subscribers.**
           - Relación con Ítem 17: Si info contractual a NO interviniente (Ítem 4 Fail por persona incorrecta), fallo principal en Ítem 17.
           """},
    "5": { "name": "Motivo llamada-identificar expediente. Prescripción", "complexity": "MEDIO", "description": "...",
           "rules_detail": """
           Manejo correcto de información de prescripción o productos/cedente/importes.
           - La información de referencia está en **k_plus_data_snapshot**: 'flag_argumentario_prescripcion' (dict con 'flag_active' y 'last_update_date'), 'is_prescription_flag_recent' (bool calculado), 'producto', 'cedente', 'importe_deuda'.
           - Regla adicional para determinar si CORRESPONDE hacer prescripción:
             - Si **k_plus_data_snapshot.flag_argumentario_prescripcion.flag_active** es true Y **k_plus_data_snapshot.is_prescription_flag_recent** es true => NO corresponde realizar prescripción, solo identificar motivo llamada informando **k_plus_data_snapshot.producto**, **k_plus_data_snapshot.cedente**, **k_plus_data_snapshot.importe_deuda**.
             - Si **k_plus_data_snapshot.flag_argumentario_prescripcion.flag_active** es false O **k_plus_data_snapshot.is_prescription_flag_recent** es false => SÍ corresponde realizar prescripción. En este caso, el agente debe usar el texto de prescripción (no proporcionado aquí, pero asume que implica usar/mencionar) y los campos: **k_plus_data_snapshot.producto**, **k_plus_data_snapshot.cedente**, **k_plus_data_snapshot.importe_deuda**.
           - Si aplica prescripción: debe ajustarse fielmente al texto facilitado (no provisto) y con datos (**producto, cedente, importe_deuda**) correctos. -> **Verificar si el agente usa estos datos de k_plus_data_snapshot y si los menciona correctamente si CORRESPONDÍA hacer prescripción/informar motivo.**
           - Si expediente NO tiene flag O flag > 3 meses Y hace prescripción: DEBE mencionar que registrará flag (no puedes verificar el registro real). -> **Verificar si k_plus_data_snapshot.flag_argumentario_prescripcion.flag_active es false o k_plus_data_snapshot.is_prescription_flag_recent es false Y el agente menciona registrar la flag.**
           - Penalizar si se facilitan datos erróneos (**producto, cedente, importe_deuda** de k_plus_data_snapshot) en la transcripción.
           - Si error corregido MÁS TARDE: NO penaliza (Pass). -> **Verificar si el agente rectifica el dato erróneo en la transcripción.**
           - Excepciones: Al hacer prescripción NO indica tipo interviniente ("usted tiene un préstamo") pero es cliente principal (ej. cliente dice "tengo una tarjeta" y es único cliente - asume esta información si no está en k_plus_data_snapshot si es relevante): NO penaliza (Pass).
           - Relación con Ítem 14: Si datos erróneos en prescripción y error mantenido, penalización en Ítem 5, NO Ítem 14. Tu tarea es identificar el error y si se corrigió DENTRO de esta llamada.
           """},
    "6": { "name": "Confirmar datos contacto (dirección, teléfono, email)", "complexity": "MEDIO", "description": "...",
            "rules_detail": """
            Valora si se confirman COMPLETOS todos los datos de contacto marcados "VALIDO" en K+.
            - Los datos de referencia son las listas de contactos válidos en **k_plus_data_snapshot**: 'addresses_valid_0_2', 'phones_valid_0_2', 'emails_valid_0_2'. Cada entrada en estas listas tiene 'address', 'phone_number', o 'email' y 'valid_code' (0 o 2).
            - Válido SÓLO si el agente confirma, de manera COMPLETA, **TODOS** los contactos presentes en **k_plus_data_snapshot.addresses_valid_0_2**, **k_plus_data_snapshot.phones_valid_0_2**, Y **k_plus_data_snapshot.emails_valid_0_2** en la transcripción.
            - EXCEPCIONES (gestionadas en código, LLM no evalúa si aplica): NO valora llamadas < 4 minutos. NO valora si incidencias K+ reportadas.
            - Relación con Ítem 21: Si por operativa NO toca confirmar datos pero se hace de forma incompleta, o si hay un envío SMS/Mail y se confirma solo el telf/mail usado dejando otros sin confirmar, se evaluaría en Ítem 21, NO en Ítem 6. Si hay envío SMS/Mail y no se confirman datos pero YA se confirmaron el último mes, se valora en Ítem 21. Si identificas una de estas situaciones en la transcripción (confirmación incompleta de datos 'VALIDO' existentes cuando SÍ tocaba confirmar), marca como Fail en Ítem 6 y en la razón indica la situación.
            - Relación con Ítem 7: Solicitar datos adicionales DESPUÉS de confirmar datos sistema -> Ítem 7, NO Ítem 6.
            """},
    "7": { "name": "Solicitar datos contacto adicionales", "complexity": "BAJO", "description": "...",
            "rules_detail": """
            Evalúa si se solicitan datos de contacto adicionales (teléfono, mail) cuando corresponde.
            - Los datos de referencia son los conteos de contactos válidos en **k_plus_data_snapshot**: 'numero_telefonos_total_validos' y 'numero_emails_total_validos'.
            - La solicitud de datos adicionales (teléfono fijo/móvil y email si no tenemos uno) debe hacerse en todas nuestras confirmaciones y/o actualizaciones de datos (**k_plus_data_snapshot.corresponds_update** es true).
            - SOLO se valora la SOLICITUD en la transcripción, no la correcta incorporación a BBDD (eso es Ítem 20).
            - Se deben solicitar adicionales SI:
              - 'numero_telefonos_total_validos' en **k_plus_data_snapshot** es 0 O 1.
              - Y/O 'numero_emails_total_validos' en **k_plus_data_snapshot** es 0.
            - Excepciones (marca N/A o Pass si aplica):
              - NO se valorará si 'numero_telefonos_total_validos' > 1 AND 'numero_emails_total_validos' > 1 en **k_plus_data_snapshot**.
              - NO se valorará si **k_plus_data_snapshot.corresponds_update** es false (No corresponde confirmación/actualización de datos).
              - La operativa NO aplica para autorizados expresos (**k_plus_data_snapshot.is_authorised_express** es true).
            - Relación con Ítem 20: Correcta incorporación a BBDD -> Ítem 20. Este ítem es SOLO sobre la SOLICITUD.
            """},
    # Ítems Críticos Individuales
    "17": { "name": "Tratamiento de la información con 3os", "complexity": "ALTO", "description": "...",
            "rules_detail": """
            Protección de información personal/contractual, evitando facilitarla a personas NO autorizadas.
            - Penaliza facilitar info a NO autorizados.
            - Los datos de referencia incluyen: **k_plus_data_snapshot.abogado_personado** (bool) y si el interviniente ESTÁ presente en la llamada a 3 (**k_plus_data_snapshot.intervener_present_in_3way** bool).
            - EXCEPCIONES (N/A si aplica): Si el interviniente ESTÁ presente en la llamada a 3 (**k_plus_data_snapshot.intervener_present_in_3way** es true). Si abogados personados en K+ (**k_plus_data_snapshot.abogado_personado** es true), NO necesita autorización (Pass si se dio info a abogado personado).
            - Relación con otros ítems: Sin relación directa relevante.
            """},
    "20": { "name": "Alta / Asignación datos (dirección teléfonos, etc.)", "complexity": "BAJO", "description": "...",
            "rules_detail": """
            Evalúa si el agente habla de dar de alta un dato nuevo necesario, o si discute un dato que NO debe darse de alta, **Y si la evidencia en k_plus_data_snapshot sugiere que la acción se realizó correctamente o incorrectamente si se discutió en la llamada.**
            - Los datos de referencia de K+ incluyen: Listas de contactos por VALID (0/2, 2) y la lista de teléfonos VALID=6/SOURCE=82 (**k_plus_data_snapshot.addresses_valid_2**, **k_plus_data_snapshot.phones_valid_2**, **k_plus_data_snapshot.emails_valid_2**, **k_plus_data_snapshot.phones_valid_6_source_82**).
            - Penaliza cuando se habla de dar de alta dato nuevo necesario que NO SE HACE (según conv.) O se habla de dar de alta dato que POR OPERATIVA NO DEBE incorporarse (ej. Comisaría).
            - **VERIFICAR en k_plus_data_snapshot:**
              - Si el agente habla de registrar un **dato nuevo** (ej. nuevo teléfono dado por cliente en transcripción): Verificar si un dato con similar contenido aparece en **k_plus_data_snapshot.phones_valid_2**, **addresses_valid_2**, o **emails_valid_2**. Su presencia sugiere un registro VALID=2. Su ausencia o presencia en VALID=0/otra lista podría sugerir no se registró o no se registró con VALID=2.
              - Si se discute dar de alta un dato que POR OPERATIVA NO DEBE incorporarse (ej. teléfono de Comisaría, Juzgado): Verificar si un teléfono mencionado en la transcripción aparece en la lista **k_plus_data_snapshot.phones_valid_6_source_82**. Si aparece, penaliza. Si el agente habla de registrarlo y aparece aquí, penaliza.
            - Si discute alta dato nuevo con error evidente en conv. (ej. mencionar un CP erróneo para una dirección), penaliza, **y si la evidencia en k_plus_data_snapshot (ej. en addresses_valid_2) confirma un registro erróneo**.
            - Relación con Ítem 21: Ciertas situaciones de alta con errores o datos no permitidos pueden valorarse en Ítem 21. En POC, si conversación muestra intento alta no permitido (teléfono de Comisaría, etc.) o discutir un alta con error evidente (ej. CP erróneo) relacionado con un dato *nuevo*, marca Fail en Ítem 20 y explica la razón. Si se habló de un alta nueva y no aparece en las listas VALID=2 en k_plus_data_snapshot, marca Fail en Ítem 20.
            """},
    "26": { "name": "Ninguna amenaza o intimidación, ironía, frases inadecuadas. Sin provocaciones, sin juicios de valor", "complexity": "ALTO", "description": "...",
            "rules_detail": """
            Agente mantiene comportamiento respetuoso, profesional, SIN dañar imagen compañía.
            - Evitar: NINGUNA amenaza/intimidación, NINGUNA ironía, NINGUNA frase inadecuada, SIN provocaciones, SIN juicios de valor. Enfócate en violaciones CLARAS Y GRAVES que dañan la imagen.
            - Relación con Ítem 23 (Comunicación general): Mayoría problemas tono/comunicación penalizan Ítem 23. SOLO Ítem 26 si acción/lenguaje DAÑA CLARAMENTE IMAGEN o trato CRITICAMENTE INAPROPIADO/IRRESPECTUOSO según puntos listados.
            """},
    # ... COPIA LAS REGLAS RESTANTES ADAPTADAS ...
}


# --- Estructura de Evaluación (Define grupos e ítems individuales) ---
# Esta estructura define QUÉ se evalúa en CADA llamada al LLM.
EVALUATION_STRUCTURE = {
    "inicio_llamada_group": {
        "item_ids": ["1", "2", "3", "4", "5", "6", "7"],
        # Template para un grupo de ítems
        "prompt_template": """
        Eres un evaluador de calidad de llamadas experto para EOS Spain. Analiza la siguiente transcripción y los datos proporcionados para determinar si el agente cumplió con los siguientes ítems del bloque "Inicio Llamada": {item_ids}.

        **Datos de la Llamada:**
        Tipo de Llamada (obtenido de origen): {call_type}
        Duración (mins): {call_duration}
        Wrap-up Code del Agente: {wrap_up_code}
        Info Wrap-up Code: {wrap_up_info}
        **Estado de Datos en K+ (post-llamada):** {k_plus_data_snapshot}
        Transcripción, con cada línea etiquetada como [A] (Agente) o [I] (Interlocutor): ```{transcript}```

        **Reglas de Evaluación para cada Ítem:**
        {all_items_rules_detail}

        Evalúa cada uno de los ítems listados ({item_ids}) estrictamente basándote en la transcripción, los DATOS DE K+ (k_plus_data_snapshot) y las reglas proporcionadas para CADA ÍTEM. Para el Ítem 6, ten en cuenta las excepciones por duración o incidencia.

        Formato de Salida: Devuelve una lista de objetos JSON, uno por cada ítem evaluado en este grupo. CADA objeto JSON debe tener las claves "item_id", "result" ("Pass" | "Fail" | "N/A"), "reason" (explicación breve, citando evidencia de transcripción O datos de K+) y "transcript_segment" (fragmento relevante).

        ```json
        [
          {{
            "item_id": "1",
            "result": "Pass" | "Fail" | "N/A",
            "reason": "...",
            "transcript_segment": "..."
          }},
          # ... y así para todos los ítems del grupo (2, 3, 4, 5, 6, 7)
        ]
        ```
        Asegúrate de que la salida sea una lista de objetos JSON válida.
        """,
    },
    "item_17_individual": {
        "item_ids": ["17"],
         # Template para un ítem individual
        "prompt_template": """
        Eres un evaluador de calidad de llamadas experto para EOS Spain. Analiza la siguiente transcripción y datos para evaluar el Ítem 17: Tratamiento de la Información con Terceros.

        **Datos de la Llamada:**
        Transcripción, con cada línea etiquetada como [A] (Agente) o [I] (Interlocutor): ```{transcript}```
        Tipo de Llamada (obtenido de origen): {call_type}
        **Estado de Datos en K+ (post-llamada):** {k_plus_data_snapshot}

        **Reglas de Evaluación (Ítem 17):**
        {all_items_rules_detail}

        Evalúa estrictamente basándote en la transcripción y los DATOS DE K+ (k_plus_data_snapshot). Enfócate en si se reveló información personal o contractual a alguien que no debía recibirla, considerando las excepciones basadas en **k_plus_data_snapshot.abogado_personado** o si el interviniente ESTÁ presente en la llamada a 3 (según el flag **k_plus_data_snapshot.intervener_present_in_3way** si se proporciona).
        Formato de Salida (JSON):
        ```json
        {{
          "item_id": "17",
          "result": "Pass" | "Fail" | "N/A",
          "reason": "Breve explicación, indicando si se facilitó información a un tercero no autorizado o no, citando evidencia de transcripción O datos de K+.",
          "transcript_segment": "Fragmento relevante (si aplica)"
        }}
        ```
        Asegúrate de que la salida sea JSON válido. Marca N/A si la regla de excepción aplica según los datos.
        """,
    },
     "item_20_individual": {
         "item_ids": ["20"],
          # Template para un ítem individual
         "prompt_template": """
        Eres un evaluador de calidad de llamadas experto para EOS Spain. Analiza la siguiente transcripción y los datos proporcionados para determinar si el agente cumplió con el Ítem 20: Discusión/Intento de Alta/Asignación de Datos Nuevos.

        **Datos de la Llamada:**
        Transcripción, con cada línea etiquetada como [A] (Agente) o [I] (Interlocutor): ```{transcript}```
        Tipo de Llamada (obtenido de origen): {call_type}
        **Estado de Datos en K+ (post-llamada):** {k_plus_data_snapshot}

        **Reglas de Evaluación (Ítem 20):**
        {all_items_rules_detail}

        Evalúa estrictamente basándote en la transcripción Y los DATOS DE K+ (k_plus_data_snapshot). Identifica si el agente discute o intenta dar de alta UN DATO *NUEVO* (dirección, teléfono, email, etc.) o si discute dar de alta un dato que POR OPERATIVA NO SE DEBE registrar (ej: teléfono de Comisaría). **CRUCIALMENTE, USA k_plus_data_snapshot para verificar si la acción de regi



Con estos cambios, el archivo item_rules.py ahora tiene sus prompts y descripciones de reglas actualizadas para hacer referencia a la nueva estructura detallada de datos de K+ que se obtiene de la base de datos.
Recuerda que deberás ir iterando y refinando estos prompts a medida que pruebes el pipeline y evalúes la precisión de los resultados generados por el LLM. La clave es ser lo más explícito posible al describirle al modelo dónde encontrar la información en el k_plus_data_snapshot y cómo aplicar las reglas usando esa información.
