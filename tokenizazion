Okay, entiendo la necesidad de estimar los costes para un volumen de producción potencial. El coste de los LLMs se basa principalmente en la cantidad de tokens enviados como entrada (prompt) y recibidos como salida (respuesta del modelo). Para hacer esta estimación, podemos integrar la contabilización de tokens en el proceso.
La mejor manera de contabilizar tokens de forma precisa es utilizando la librería de tokenización específica del modelo que estás usando. Para los modelos de OpenAI (incluyendo los desplegados en Azure), la librería recomendada es tiktoken.
Pasos para la Contabilización de Tokens:
 * Instalar tiktoken: Si aún no la tienes, debes añadirla a tus dependencias.
 * Implementar Conteo en evaluate_with_llm: Modificaremos la función que llama a la API (evaluate_with_llm en src/evaluation/evaluator.py) para contar los tokens del prompt antes de enviarlo y los tokens de la respuesta al recibirla.
 * Recolectar los Conteos: Modificaremos main_workflow.py para agregar los conteos de tokens de cada llamada al LLM (recuerda que hay una llamada por cada grupo/ítem individual en EVALUATION_STRUCTURE).
 * Reportar los Resultados: Añadiremos la información de tokens al resultado final guardado por results_handler.py.
1. Actualizar project_root/requirements.txt
Añade tiktoken:
openai>=1.0.0
python-dotenv>=1.0.0
PyYAML>=6.0
requests>=2.0.0
pytz>=2023.0
tiktoken>=0.5.0 # Añadir
# Si usas pandas para K+ data:
# pandas>=2.0.0

Instala la nueva dependencia: pip install -r requirements.txt.
2. Actualizar src/evaluation/evaluator.py (Implementar Conteo)
Modificaremos evaluate_with_llm para usar tiktoken y retornar los conteos de tokens de entrada y salida junto con los resultados.
import json
# Importar tiktoken para contar tokens
import tiktoken
# Importar config_manager para obtener config API
from src.config import config_manager
# Importar reglas y estructura de evaluación
from .item_rules import ITEM_RULES, EVALUATION_STRUCTURE
# Importar AzureOpenAI client
from openai import AzureOpenAI # Asegurarse de que el cliente está configurado

# Configurar el cliente OpenAI (asegúrate de que usa las credenciales y endpoint de config_manager)
# Esto asume que AZURE_OPENAI_ENDPOINT y AZURE_OPENAI_KEY están en .env o variables de entorno
openai_client = AzureOpenAI(
    azure_endpoint=config_manager.get_env("AZURE_OPENAI_ENDPOINT"),
    api_key=config_manager.get_env("AZURE_OPENAI_KEY"),
    api_version=config_manager.get("openai.api_version")
)

# Obtener el encoding para el modelo usado
# Esto debe coincidir con el modelo desplegado en Azure
# Para la mayoría de los modelos recientes de OpenAI, 'cl100k_base' es el encoding correcto.
# Verifica la documentación de Azure OpenAI para tu deployment específico si tienes dudas.
try:
    encoding = tiktoken.get_encoding("cl100k_base") # Usar el encoding común para gpt-4, gpt-3.5-turbo, etc.
    # Si tu deployment usa un modelo con un encoding diferente, ajusta aquí.
    # encoding = tiktoken.encoding_for_model("nombre-del-modelo-desplegado") # Alternativa si conoces el nombre exacto del modelo
except Exception as e:
    print(f"Error loading tiktoken encoding: {e}. Token counting may not be accurate.")
    encoding = None # Proceder sin conteo preciso si falla

def count_tokens(text: str) -> int:
    """Cuenta tokens usando el encoding configurado."""
    if encoding is None or not isinstance(text, str):
        return 0 # No contar si el encoding falló o la entrada no es string
    return len(encoding.encode(text))


def evaluate_with_llm(evaluation_key: str, transcript: str, call_metadata: dict) -> tuple[list[dict], int, int]:
    """
    Evalúa un grupo de ítems o un ítem individual utilizando el LLM y contabiliza tokens.

    Args:
        evaluation_key (str): La clave del grupo o ítem individual en EVALUATION_STRUCTURE.
        transcript (str): La transcripción completa de la llamada.
        call_metadata (dict): Metadatos de la llamada (incluyendo k_plus_data_snapshot).

    Returns:
        tuple[list[dict], int, int]: Una tupla conteniendo:
            - list[dict]: Lista de diccionarios con los resultados de evaluación.
            - int: Tokens de entrada (prompt).
            - int: Tokens de salida (respuesta).
            Retorna una lista vacía y (0, 0) si falla la llamada al API o parsing.
    """
    evaluation_config = EVALUATION_STRUCTURE.get(evaluation_key)
    if not evaluation_config:
        print(f"Error: Evaluation config not found for key {evaluation_key}.")
        return [{"item_id": "Error", "result": "Error", "reason": f"Configuración no encontrada para {evaluation_key}", "transcript_segment": ""}], 0, 0

    item_ids = evaluation_config["item_ids"]
    prompt_template = evaluation_config["prompt_template"]

    # Compilar las reglas detalladas para los ítems de este grupo/individual
    all_items_rules_detail = ""
    for item_id in item_ids:
        if item_id in ITEM_RULES:
            all_items_rules_detail += f"\n--- Ítem {item_id}: {ITEM_RULES[item_id]['name']} ---\n"
            all_items_rules_detail += ITEM_RULES[item_id]['rules_detail'] + "\n"
        else:
            all_items_rules_detail += f"\n--- Ítem {item_id}: Reglas NO DISPONIBLES ---\n"

    # Preparar datos para el prompt. Asegurarse de incluir k_plus_data_snapshot.
    # NOTA: Asegúrate de que esta preparación de datos sea robusta para todos los prompts
    # definidos en EVALUATION_STRUCTURE.
    prompt_data = {
        "item_ids": ", ".join(item_ids),
        "transcript": transcript,
        "all_items_rules_detail": all_items_rules_detail,
        # Incluir metadatos y el snapshot de K+. Convertir snapshot a string JSON.
        "call_type": call_metadata.get("call_type", "Desconocido"),
        "call_duration": call_metadata.get("duration_minutes", 0), # Usar duración en minutos si está disponible
        "k_plus_incident": call_metadata.get("k_plus_incident", False),
        "intervener_present_in_3way": call_metadata.get("intervener_present_in_3way", False),
        "lawyer_personado": call_metadata.get("lawyer_personado", False),
        # Combinar contactos válidos para prompt 6 si es necesario (adaptar si la estructura de K+ snapshot cambió)
        "k_plus_valid_contacts": call_metadata.get("k_plus_data_snapshot", {}).get("telefonos_validos", []) + call_metadata.get("k_plus_data_snapshot", {}).get("emails_validos", []) + ([call_metadata.get("k_plus_data_snapshot", {}).get("direccion_valido")] if call_metadata.get("k_plus_data_snapshot", {}).get("direccion_valido") else []),
        "has_prescription_flag": call_metadata.get("k_plus_data_snapshot", {}).get("tiene_flag_argumentario_prescripcion", False),
        "prescription_flag_date": call_metadata.get("k_plus_data_snapshot", {}).get("fecha_flag_argumentario_prescripcion", "N/A"),
        "is_prescription_flag_recent": call_metadata.get("is_prescription_flag_recent", False), # Dato calculado en data_loader
        "k_plus_phone_count": call_metadata.get("k_plus_data_snapshot", {}).get("numero_telefonos_total_en_k", 0), # Usar total counts para Item 7
        "k_plus_email_count": call_metadata.get("k_plus_data_snapshot", {}).get("numero_emails_total_en_k", 0), # Usar total counts para Item 7
        "corresponds_update": call_metadata.get("corresponds_update", False), # Dato a nivel de llamada/metadata
        "is_authorised_express": call_metadata.get("is_authorised_express", False), # Dato a nivel de llamada/metadata
        # Pasar el snapshot completo de K+ como string JSON
        "k_plus_data_snapshot": json.dumps(call_metadata.get("k_plus_data_snapshot", {}), ensure_ascii=False)
    }


    # Formatear el prompt
    try:
        prompt = prompt_template.format(**prompt_data)
    except KeyError as e:
         print(f"Error formatting prompt for evaluation key {evaluation_key}: Missing key {e}")
         # Retorna un resultado de error para cada ítem en el grupo/individual
         error_results = [{"item_id": item_id, "result": "Error", "reason": f"Error formatting prompt: Missing data {e}", "transcript_segment": ""} for item_id in item_ids]
         return error_results, 0, 0


    # Construir los mensajes para la API
    system_message = "You are an AI assistant specialized in analyzing call center transcripts for quality assurance based on predefined criteria for EOS Spain. Your task is to evaluate the specified compliance items based *strictly* on the provided transcript, call metadata, and evaluation rules. Use the K+ data snapshot provided to verify system state when rules require it. Respond ONLY with the requested JSON object or list of JSON objects."
    user_message = prompt

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    # --- Contabilizar tokens de entrada ---
    # Contar tokens en los mensajes que se enviarán
    # La función count_tokens de tiktoken para mensajes de chat es más compleja que simple conteo de texto.
    # Incluye tokens especiales por rol, etc.
    # Una aproximación simple es sumar los tokens de los contenidos:
    input_tokens = count_tokens(system_message) + count_tokens(user_message)
    # Una aproximación más precisa con tiktoken (requiere la librería instalada y el nombre exacto del modelo):
    # try:
    #     input_tokens = len(tiktoken.encoding_for_model(config_manager.get("openai.deployment_name")).encode_messages(messages))
    # except Exception as e:
    #     print(f"Warning: Could not use tiktoken.encoding_for_model for input token counting: {e}. Falling back to simple content sum.")
    #     input_tokens = count_tokens(system_message) + count_tokens(user_message)


    try:
        # print(f"Sending prompt for {evaluation_key} ({item_ids})... Input Tokens: {input_tokens}") # Debugging print
        response = openai_client.chat.completions.create(
            model=config_manager.get("openai.deployment_name"),
            messages=messages,
            temperature=0,
            # max_tokens=... # Si no se especifica, el modelo decide (hasta el límite del modelo)
        )
        llm_output_string = response.choices[0].message.content.strip()

        # --- Contabilizar tokens de salida ---
        output_tokens = count_tokens(llm_output_string)
        # O usando la info de uso de la respuesta (más precisa si la API lo proporciona):
        # if response.usage:
        #      input_tokens_api = response.usage.prompt_tokens
        #      output_tokens_api = response.usage.completion_tokens
        #      # Usar estos si están disponibles, son más precisos que el conteo manual
        #      # Puedes loguear o retornar ambos para comparación/verificación
        #      # Para simplicidad ahora, retornamos el conteo manual, pero ten en cuenta response.usage

        # Limpiar posibles marcadores de código JSON
        if llm_output_string.startswith("```json"):
             llm_output_string = llm_output_string[len("```json"):].strip()
             if llm_output_string.endswith("```"):
                 llm_output_string = llm_output_string[:-len("```")].strip()

        # Intentar parsear la respuesta.
        try:
            parsed_output = json.loads(llm_output_string)
        except json.JSONDecodeError:
             print(f"Error decoding JSON from LLM output for {evaluation_key}:\n{llm_output_string}")
             # Retorna resultados de error si falla el parsing
             error_results = [{"item_id": item_id, "result": "Error", "reason": f"JSON decoding error: {llm_output_string[:150]}...", "transcript_segment": llm_output_string} for item_id in item_ids]
             return error_results, input_tokens, output_tokens # Retorna tokens contados incluso en error


        # Validar y estandarizar la salida (similar a antes)
        results = []
        if evaluation_key.endswith("_group"): # Esperamos una lista para grupos
            if isinstance(parsed_output, list):
                for item_result in parsed_output:
                    if isinstance(item_result, dict) and item_result.get('item_id') in item_ids and 'result' in item_result and 'reason' in item_result:
                         if 'transcript_segment' not in item_result: item_result['transcript_segment'] = ""
                         results.append(item_result)
                    else:
                         # Handle malformed individual item in the list
                         item_id_from_output = item_result.get('item_id', 'Unknown')
                         if item_id_from_output not in item_ids: item_id_from_output = "Unknown"
                         results.append({"item_id": item_id_from_output, "result": "Error", "reason": f"Malformed item output from LLM: {json.dumps(item_result)[:100]}...", "transcript_segment": ""})

                # Check for missing items in the returned list for the group
                returned_item_ids = {res.get('item_id') for res in results if isinstance(res, dict)}
                missing_item_ids = set(item_ids) - returned_item_ids
                for missing_id in missing_item_ids:
                     print(f"Warning: LLM did not return result for item {missing_id} in group {evaluation_key}. Adding error result.")
                     results.append({"item_id": missing_id, "result": "Error", "reason": "LLM did not return result for this item in the group.", "transcript_segment": ""})

            else:
                # Handle case where group response is not a list
                print(f"Warning: LLM output for group {evaluation_key} is not a list:\n{llm_output_string}")
                results = [{"item_id": item_id, "result": "Error", "reason": f"LLM output for group not list: {llm_output_string[:150]}...", "transcript_segment": llm_output_string} for item_id in item_ids]

        else: # Esperamos un diccionario para ítems individuales
             if isinstance(parsed_output, dict) and parsed_output.get('item_id') == item_ids[0] and 'result' in parsed_output and 'reason' in parsed_output:
                  if 'transcript_segment' not in parsed_output: parsed_output['transcript_segment'] = ""
                  results.append(parsed_output)
             else:
                  print(f"Warning: LLM output for individual item {evaluation_key} is not in expected format:\n{llm_output_string}")
                  results.append({"item_id": item_ids[0], "result": "Error", "reason": f"LLM output format error for individual item: {llm_output_string[:150]}...", "transcript_segment": llm_output_string})

        return results, input_tokens, output_tokens # Retorna resultados Y conteos de tokens

    except Exception as e:
        print(f"Error calling Azure OpenAI API for {evaluation_key}: {e}")
        # Retornar resultados de error si falla la API
        error_results = [{"item_id": item_id, "result": "Error", "reason": f"API error: {e}", "transcript_segment": ""} for item_id in item_ids]
        return error_results, input_tokens, 0 # Retorna input_tokens contados, output_tokens es 0 si API falla

3. Actualizar src/main_workflow.py (Recolectar y Pasar Conteos)
Modificaremos el bucle de evaluación para recoger los tokens retornados por evaluate_with_llm y los agregaremos. También añadiremos estos conteos a los resultados finales antes de guardarlos.
import os
import glob
from datetime import datetime, timedelta
from pytz import timezone
import pytz

from src.config import config_manager
from src.data_acquisition import downloader
from src.audio_processing import whisper_processor
from src.data_preparation import data_loader, transcript_filter
from src.evaluation import evaluate_with_llm, apply_post_processing, EVALUATION_STRUCTURE
from src.results import results_handler

def main():
    """
    Función principal que ejecuta el pipeline de evaluación.
    """
    print("--- Starting Automated Call Evaluation Pipeline ---")

    # 1. Cargar Configuración
    config = config_manager

    data_paths = config.get("data_paths")
    pipeline_config = config.get("pipeline")
    genesys_config = config.get("genesys_cloud")

    if not data_paths or not pipeline_config or not genesys_config:
        print("Error: Failed to load essential configuration. Exiting.")
        return

    # Asegurar directorios existen
    for path_key in data_paths.values():
        if isinstance(path_key, str):
             os.makedirs(path_key, exist_ok=True)

    # --- Inicializar contadores globales de tokens ---
    total_input_tokens = 0
    total_output_tokens = 0
    # Diccionario para conteos por evaluation_key (opcional)
    token_counts_by_eval_key = {key: {"input": 0, "output": 0} for key in EVALUATION_STRUCTURE.keys()}


    # 2. Adquisición de Datos (Genesys Transcriptions Download)
    print("\nStep 2: Data Acquisition (Genesys Transcriptions Download)")
    download_date_range_config = pipeline_config.get("download_date_range")

    raw_transcript_filepaths = [] # Inicializar siempre la lista


    if download_date_range_config and download_date_range_config.get("start_datetime") and download_date_range_config.get("end_datetime"):
        start_datetime_str = download_date_range_config["start_datetime"]
        end_datetime_str = download_date_range_config["end_datetime"]

        try:
             raw_transcript_filepaths = downloader.download_transcriptions_batch(start_datetime_str, end_datetime_str)

        except Exception as e:
             print(f"An error occurred during the download step: {e}")
             # raw_transcript_filepaths seguirá siendo []


        if raw_transcript_filepaths:
             print(f"Successfully downloaded {len(raw_transcript_filepaths)} raw transcript files.")
        else:
             print("No raw transcripts were downloaded successfully for the specified date range.")
    else:
        print("Download datetime range not specified in config/settings.yaml or is incomplete. Skipping download step.")
        # Si se salta la descarga, asumimos que los archivos JSON ya están en data/raw_transcripts/
        print(f"Assuming raw transcripts (JSON) are already in {data_paths['raw_transcripts']}.")
        raw_transcript_filepaths = glob.glob(os.path.join(data_paths["raw_transcripts"], "*.json"))
        print(f"Found {len(raw_transcript_filepaths)} existing raw transcript files.")

    if not raw_transcript_filepaths:
         print("No raw transcript files available after attempted download or finding existing. Exiting pipeline.")
         # Reportar conteos antes de salir si algo se procesó (aunque aquí sería 0)
         report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)
         return


    # 3. Filtrado de Transcripciones Descargadas (por Wrap-up Code)
    print("\nStep 3: Filtering Raw Transcripts")
    filtered_calls_info = transcript_filter.filter_transcriptions(raw_transcript_filepaths)

    if not filtered_calls_info:
        print("No transcripts passed the filter or none were included (if filter disabled). Exiting pipeline.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key) # Reportar antes de salir
        return


    # 4. Procesamiento de Audio (Whisper) - Condicional
    # Esto se activa si use_whisper es True y hay archivos de audio correspondientes
    # a las transcripciones *filtradas*.
    use_whisper = pipeline_config.get("use_whisper", False)

    transcriptions_for_evaluation_info = filtered_calls_info # Por defecto, usamos las raw filtradas info

    if use_whisper:
         print("\nStep 4: Audio Processing (Whisper) - PLACEHOLDER")
         print("This step would process downloaded audio files corresponding to filtered transcripts.")
         # Lógica futura (ver implementación anterior) para actualizar transcriptions_for_evaluation_info
         # para que apunte a los archivos .txt de Whisper y marque 'used_whisper': True


    else:
        print("\nStep 4: Audio Processing (Skipped as per configuration)")
        print(f"Using raw filtered transcripts from {data_paths['raw_transcripts']} for evaluation.")
        # Asegurar que la info_para_evaluacion tenga el flag use_whisper=False si no se intentó
        for call_info in transcriptions_for_evaluation_info:
             call_info['used_whisper'] = False


    # 5. Carga de Datos para Evaluación
    print(f"\nStep 5: Data Loading for Evaluation ({len(transcriptions_for_evaluation_info)} calls)")

    all_evaluation_results_flat = [] # Lista plana para todos los resultados de todos los ítems/llamadas


    if not transcriptions_for_evaluation_info:
        print("No calls identified for evaluation after filtering and optional Whisper step. Exiting.")
        report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key) # Reportar antes de salir
        return


    print(f"Identified {len(transcriptions_for_evaluation_info)} calls to process for evaluation.")
    for call_info in transcriptions_for_evaluation_info:
        call_id = call_info['conversationId']
        raw_transcript_filepath = call_info['filepath']
        use_whisper_for_this_call = call_info.get('used_whisper', False)


        # Cargar datos para la llamada actual
        call_data = data_loader.load_call_data_for_evaluation(
            call_id,
            raw_transcript_filepath,
            use_whisper=use_whisper_for_this_call
        )


        if not call_data or not call_data.get("transcript"):
            print(f"Skipping evaluation for call {call_id} due to data loading error or missing transcript content.")
            all_item_ids = [id for group in EVALUATION_STRUCTURE.values() for id in group['item_ids']]
            error_results = [{"call_id": call_id, "conversationId": call_info.get('conversationId'), "communicationId": call_info.get('communicationId'), "item_id": item_id, "result": "Error", "reason": "Failed to load transcript or data.", "transcript_segment": ""} for item_id in all_item_ids]
            all_evaluation_results_flat.extend(error_results)
            continue

        # 6. Evaluación con LLM
        print(f"\nStep 6: Evaluation for call {call_id}")
        initial_results_for_call = []
        eval_structure_keys = pipeline_config.get("items_to_evaluate")
        if not eval_structure_keys:
            eval_structure_keys = list(EVALUATION_STRUCTURE.keys()) # Convertir a lista para iterar
            print("No specific items_to_evaluate defined in config. Evaluating all defined groups/items.")
        else:
             valid_eval_keys = [k for k in eval_structure_keys if k in EVALUATION_STRUCTURE]
             if len(valid_eval_keys) != len(eval_structure_keys):
                  print(f"Warning: Some evaluation keys from config were not found in EVALUATION_STRUCTURE: {set(eval_structure_keys) - set(valid_eval_keys)}")
             eval_structure_keys = valid_eval_keys


        for evaluation_key in eval_structure_keys:
             item_ids_in_group = EVALUATION_STRUCTURE[evaluation_key]["item_ids"]
             print(f"  Evaluating group/item: {evaluation_key} ({item_ids_in_group}) for call {call_id}")

             # Llamada a la función de evaluación con LLM
             # evaluate_with_llm ahora retorna resultados, input_tokens, output_tokens
             group_results, input_t, output_t = evaluate_with_llm(evaluation_key, call_data['transcript'], call_data['call_metadata'])

             # --- Acumular conteos de tokens ---
             total_input_tokens += input_t
             total_output_tokens += output_t
             if evaluation_key in token_counts_by_eval_key:
                  token_counts_by_eval_key[evaluation_key]["input"] += input_t
                  token_counts_by_eval_key[evaluation_key]["output"] += output_t
             else: # Si la clave de evaluación no estaba en la estructura inicial (ej. error), añadirla
                  token_counts_by_eval_key[evaluation_key] = {"input": input_t, "output": output_t}


             initial_results_for_call.extend(group_results)


        # 7. Post-Procesamiento
        print(f"\nStep 7: Post-Processing for call {call_id}")
        final_results_for_call = apply_post_processing(initial_results_for_call, call_data['call_metadata'])

        # --- Añadir conversationId y communicationId a cada resultado individual ---
        conversation_id_for_results = call_data['call_metadata'].get('conversationId', call_id)
        communication_id_for_results = call_data['call_metadata'].get('communicationId', call_id)

        for res in final_results_for_call:
            if 'call_id' not in res:
                 res['call_id'] = call_id
            res['conversationId'] = conversation_id_for_results
            res['communicationId'] = communication_id_for_results

        all_evaluation_results_flat.extend(final_results_for_call)


    # 8. Guardar Resultados
    print("\nStep 8: Saving Results")
    if all_evaluation_results_flat:
        # Guardar en JSON
        # Podríamos añadir los conteos totales al archivo JSON de resultados o guardarlos por separado.
        # Para simplicidad en la POC, los reportaremos por consola al final.
        results_handler.save_evaluation_results_to_json(all_evaluation_results_flat, "evaluation_output.json")
        # Guardar en CSV (si necesitas)
        # results_handler.save_evaluation_results_to_csv(all_evaluation_results_flat, "evaluation_output.csv")
    else:
        print("No evaluation results generated to save.")

    # 9. Reportar Conteo de Tokens
    print("\n--- LLM Token Usage Summary ---")
    report_token_counts(total_input_tokens, total_output_tokens, token_counts_by_eval_key)

    print("\n--- Automated Call Evaluation Pipeline Finished ---")

# Función auxiliar para reportar los conteos
def report_token_counts(total_input: int, total_output: int, counts_by_key: dict):
    """Imprime un resumen de los tokens utilizados."""
    print(f"Total Input Tokens: {total_input}")
    print(f"Total Output Tokens: {total_output}")
    print("\nTokens by Evaluation Key:")
    for key, counts in counts_by_key.items():
        print(f"  - {key}: Input = {counts['input']}, Output = {counts['output']}")
    print("\nNote: Token counts are approximate and depend on the specific model tokenizer.")
    print("Refer to Azure OpenAI pricing for your specific model deployment to estimate cost.")


# Punto de entrada del script
if __name__ == "__main__":
    main()
Con estos cambios:
 * evaluate_with_llm ahora calcula y retorna los tokens de entrada y salida para cada llamada a la API.
 * main_workflow recoge estos conteos de cada llamada a evaluate_with_llm, los acumula globalmente y por evaluation_key.
 * Al finalizar el pipeline, main_workflow llama a una nueva función report_token_counts para imprimir un resumen detallado en la consola.
Para estimar el coste, necesitarás consultar los precios específicos del modelo que tienes desplegado en Azure OpenAI. Los precios suelen ser por cada 1,000 tokens de entrada y por cada 1,000 tokens de salida, y varían significativamente entre modelos (ej. GPT-4o es generalmente más barato que GPT-4 Turbo, y mucho más barato que GPT-4 a secas; GPT-3.5-turbo es más barato).
La fórmula básica sería:
Costo Total = (Total Tokens Entrada / 1000) * Precio por 1K Tokens Entrada + (Total Tokens Salida / 1000) * Precio por 1K Tokens Salida
Por ejemplo, si GPT-4o cuesta $0.005 por 1K input tokens y $0.015 por 1K output tokens, y tu pipeline procesó 1,000,000 tokens de entrada y 500,000 tokens de salida:
Costo = (1,000,000 / 1000) * $0.005 + (500,000 / 1000) * $0.015
Costo = 1000 * $0.005 + 500 * $0.015
Costo = $5 + $7.5 = $12.5
La granularidad por evaluation_key te permitirá ver qué partes de la evaluación (el grupo de Inicio de Llamada, los ítems individuales 17, 20, 26) contribuyen más al coste y si hay oportunidades de optimización específicas.